# Jottings

### 1. EC2 standard networking

- EC2 standard networking is the default network setup for EC2 instances, offering basic connectivity with moderate speed and higher latency compared to Enhanced Networking.

### 2. EC2 Enhanced Networking

- EC2 Enhanced Networking is a feature in AWS that makes your virtual server (EC2 instance) communicate faster and more efficiently over the network. It reduces delays, increases speed, and handles more data at once, making it great for applications that need high performance, such as big data processing or gaming servers.
- EC2 Enhanced Networking supports Elastic Network Adapter (ENA) and the legacy Intel 82599 VF (up to 10 Gbps).

### 3. Elastic Network Adapter

- Elastic Network Adapter (ENA) is a special network card for AWS EC2 instances that allows them to send and receive data faster (up to 100 Gbps) with lower delays. It helps applications that need high-speed networking, like video streaming, gaming, and big data processing.

### 4. Intel 82599 Virtual Function

- Intel 82599 Virtual Function (VF) is an older network adapter option for AWS EC2 instances that allows data transfer up to 10 Gbps. It provides better performance than standard networking but is considered legacy compared to the faster ENA (Elastic Network Adapter).

### 5. Elastic Fabric Adapter (EFA)

- Elastic Fabric Adapter (EFA) is a special version of Elastic Network Adapter (ENA) that makes Linux EC2 instances communicate even faster, especially for supercomputing and machine learning. While ENA boosts network speed, EFA takes it further by reducing delays and helping servers work together more smoothly.
- Enhanced Networking and EFA (Elastic Fabric Adapter) are separate features.
- Enhanced Networking (ENA or Intel 82599 VF) improves general network speed and performance.
- EFA is built on ENA but adds extra features for supercomputing, machine learning, and HPC workloads by reducing delays and improving communication between servers.

### 6. EC2 Placement Groups

- EC2 Placement Groups are like choosing where to sit in a restaurant—whether you want to be close to friends (for fast talking), spread out (for safety), or in a balanced spot (for a mix of both).
- EC2 Placement Groups are a way to organize your EC2 instances to control how they are placed on AWS servers, helping improve speed, reduce delays, or increase reliability.

### 7. Cluster Placement Group

- Groups instances close together within a single Availability Zone for low-latency, high-speed networking.

### 8. Spread Placement Group

- In a Spread Placement Group, each instance is placed on a separate hardware (rack). These racks can be in the same or different AZs. You can have a spread group that is limited to one AZ, but if spread across multiple AZs, it allows up to 7 instances per AZ.
  -If you launch 10 instances in a Spread Placement Group across 3 AZs, AWS might place: - 4 instances in AZ-1 (each on a separate rack) - 3 instances in AZ-2 (each on a separate rack) - 3 instances in AZ-3 (each on a separate rack)

### 9. What is a Partition?

- A partition is a group of hardwares (racks) not necessarily working together as teammates or collaborating to process workloads, but serving instances within that partition. They are isolated from other partitions to prevent failures from spreading.
- They exist within a single Availability Zone. You can have multiple partitions within the same AZ.

### 10. Partition Placement Groups

- Spreads instances across separate racks (partitions) within an Availability Zone, scaling up to hundreds of instances for big data applications (e.g., Hadoop, Cassandra, Kafka).
- If you create a Partition Placement Group in AZ-1 with 3 partitions, AWS might distribute instances like this:
  - Partition 1: Uses Rack A, Rack B, Rack C
  - Partition 2: Uses Rack D, Rack E, Rack F
  - Partition 3: Uses Rack G, Rack H, Rack I

### 11. Placement groups limitations

- Cluster Placement Group → No fixed limit, but all instances must be in the same AZ. Optimized for low-latency, high-speed networking. AWS recommends using homogeneous instance types for best performance.
- Spread Placement Group → Limited to 7 instances per Availability Zone. If using multiple AZs, you can have 7 instances per AZ (e.g., 21 instances if spread across 3 AZs). Each instance is placed on a separate rack for maximum fault tolerance.
- Partition Placement Group → No strict instance limit (can scale to hundreds of instances). You can have up to 7 partitions per AZ (default limit, but can be increased). Each partition can hold many instances (AWS distributes them across racks inside that partition).

### 12. Termination Protection

- Even with Termination Protection enabled, an instance will still terminate if shut down from within the OS(i.e sudo shutdown) if its Shutdown Behavior is set to "Terminate", but Termination Protection prevents deletion from the AWS Console, CLI, or API.
- Running sudo shutdown inside an EC2 instance will stop or terminate the instance based on its Shutdown Behavior setting:
  - If Shutdown Behavior = "Stop" → The instance stops (can be restarted later).
  - If Shutdown Behavior = "Terminate" → The instance terminates (permanently deleted).

### 13. What is a vCPU?

- A vCPU (virtual Central Processing Unit) is a logical CPU assigned to an EC2 instance. It represents a portion of a physical CPU's computing power and is used to process tasks within an instance. The number of vCPUs an instance has depends on its instance type (e.g., t3.micro has 2 vCPUs, while c5.24xlarge has 96 vCPUs).
- Launching the instance in a different region helps because vCPU limits are set per region, meaning the limit you reached applies only to the current region. If you switch to another region where you haven't hit the limit, you can launch the instance there without needing an immediate limit increase. However, this may introduce latency issues if your workload requires low-latency connections between regions.

### 14. What Makes vCPU Usage Increase?

- Launching More Instances → Each instance type has a fixed number of vCPUs, so more instances = more vCPUs used.
- Using Larger Instance Types → Bigger instances (e.g., m5.8xlarge) have more vCPUs than smaller ones (e.g., m5.large).
- Increasing Auto Scaling → If an Auto Scaling Group adds instances, the total vCPU count increases.
- Running On-Demand or Spot Instances → AWS tracks only running instances for vCPU limits (stopped instances don't count).

### 15. InstanceLimitExceeded error(your fault)

- The #InstanceLimitExceeded error occurs when you exceed the maximum allowed vCPUs for running instances in a region, which is based on AWS's vCPU-based limits for On-Demand and Spot instances.

### 16. InsufficientInstanceCapacity(aws fault)

- The #InsufficientInstanceCapacity error occurs when AWS lacks enough On-Demand capacity in a specific Availability Zone, and it can be resolved by waiting and retrying, splitting large requests into smaller ones, selecting a different instance type, or launching in another AZ.

### 17. Instance Terminates Immediately

- The Instance Terminates Immediately issue occurs when an EC2 instance goes from pending to terminated due to reasons like reaching the EBS volume limit, a corrupt EBS snapshot, missing permissions for an encrypted root EBS volume, or a store-backed AMI missing required files
- The exact cause can be found in the EC2 Console → Instances → Description tab → State transition reason label.

### 18 Private Key Permissions

- 400 permissions on a .pem file mean that only the file owner can read it, preventing unauthorized access and avoiding the "Unprotected private key file" SSH error.
- Ensure the .pem file has 400 permissions to avoid the "Unprotected private key file" error.

### 19. Corrrect OS username

- Using the correct OS username (e.g., ec2-user for Amazon Linux, ubuntu for Ubuntu) is essential for successful SSH login and to avoid errors like "Host key not found", "Permission denied", or "Connection closed by [instance] port 22"

### 20. Connection timed out Error

- A "Connection timed out" error when connecting to an EC2 instance via SSH can be caused by misconfigured Security Groups (SG) or NACLs, incorrect subnet route tables, missing public IPv4, or high CPU load on the instance.

### 21. SSH vs Instance Connect

- With SSH, you can only connect to an EC2 instance if your IP address is allowed in the Security Group, blocking others. EC2 Instance Connect lets you connect without changing security rules by temporarily adding a public key for access.
- Temporarily adding a public key means that EC2 Instance Connect pushes an SSH key to the EC2 instance for a short time (usually 60 seconds), allowing you to connect without permanently storing keys or modifying security rules.

### 22. EC2 Instance Connect

- EC2 Instance Connect works by temporarily pushing an SSH key to the instance using AWS's special IP range, which does not require the instance itself to have an IP from that range.
- However, for Instance Connect to succeed, the Security Group must allow inbound SSH (port 22) from AWS's special IP range. By default, new instances usually work with Instance Connect because they inherit a Security Group that permits SSH from 0.0.0.0/0, which includes AWS's IP range.
- If you modify the inbound rules to allow only your local PC's IP, Instance Connect will fail since AWS's special IP range is blocked.
- To keep both direct SSH and Instance Connect functional, you should allow SSH from both your local IP and AWS's special IP range in the Security Group.

### 23. Instance Price Predictability

- On-Demand Instances have fixed per-second or per-hour pricing, so you always know the cost upfront without long-term commitments(Reserved) or price changes(Spot).
- Reserved Instances pricing can be unpredictable if you commit to a term but later need different instance types, regions, or fewer instances, as the upfront payment is non-refundable and locked in.
- Spot Instances pricing is unpredictable because it changes based on supply and demand, meaning the cost can go up or down at any time, and AWS can terminate the instance if the price exceeds your bid.

### 24. Instances Usage

- Recommended for short term and un-interrupted workloads, where you can't predict how the application will behave.
- You can buy and sell Reserved Instances in Reserved Instance Marketplace if you dont need them anymore.
- Convertible Reserved Instances offer up to 66% discount and allow flexibility to change the EC2 instance type, family, OS, scope, and tenancy during the reservation period.

### 25. EC2 Spot Instances

- EC2 Spot Instances offer up to 90% cost savings compared to On-Demand but can be interrupted anytime if the Spot price exceeds your bid, making them ideal for batch jobs, data analysis, image processing, and flexible workloads, but not suitable for critical jobs or databases.

### 26. EC2 Dedicated Hosts

- EC2 Dedicated Hosts provide a fully dedicated physical server for your use, supporting BYOL (Bring Your Own License) and compliance requirements, but are the most expensive option, available On-Demand or Reserved (1 or 3 years).
- BYOL (Bring Your Own License) allows you to use your existing software licenses on AWS infrastructure.

### 27. Dedicated Instances Vs Dedicated Hosts

- Dedicated Instances → AWS reserves a physical server for your account, but AWS decides instance placement, and instances may move if stopped/started.
- Dedicated Hosts → AWS reserves a physical server for your account, but YOU control where instances are placed, ensuring they stay on the same host.
- This makes Dedicated Hosts ideal for BYOL (Bring Your Own License) scenarios, compliance needs, and workloads requiring manual instance placement.

### 28. EC2 Capacity Reservations

- EC2 Capacity Reservations guarantee On-Demand instance capacity in a specific Availability Zone (AZ) for any duration, with no time commitment or discounts, and you pay the On-Demand rate even if instances are not running, making them ideal for short-term, uninterrupted workloads needing capacity assurance.

### 29. EC2 Savings Plans

- EC2 Savings Plans give you big discounts (up to 72%) if you commit to spending a fixed amount per hour ($/hour) for 1 or 3 years, allowing flexibility in instance size, OS, and tenancy, but they are limited to a specific instance family and AWS region.
- If you always use EC2 and want discounts without being locked into specific instances, Savings Plans are a great choice
- EC2 Savings Plans give discounts based on a spending commitment ($/hour) but do not guarantee capacity—you save money but still depend on available EC2 instances.
- EC2 Capacity Reservations guarantee instance availability in a specific AZ but do not provide discounts—you pay On-Demand rates even if the instances are not running.

### 30. EC2 IPv4 750-hour free tier

- Starting February 1, 2024, AWS charges $0.005 per hour (~$3.6/month) for each Public IPv4, with a 750-hour free tier for new EC2 instances in the first 12 months, but no free tier for Load Balancers or RDS.

### 31. Cancelling a Spot Request

- To terminate Spot Instances, you must first cancel the Spot Request (if it is open, active, or disabled) and then manually terminate the running Spot Instances, as cancelling a Spot Request alone does not terminate the instances.

### 32. pot Fleet

- A Spot Fleet is a group of Spot Instances (and optional On-Demand Instances) that automatically finds the best-priced instances based on your settings, using different strategies to balance cost, availability, and performance.
- A Spot Fleet selects the best instance type and Availability Zone from multiple launch pools (e.g., m5.large in us-east-1a, m5.xlarge in us-east-1b, c5.large in us-east-1c) based on price and availability to maintain capacity at the lowest cost.

### 33. Launch pools

- Launch pools in a Spot Fleet are the different instance types, operating systems, and Availability Zones that the fleet can choose from when launching instances.
- A Spot Fleet checks all the launch pools and picks the best option based on your price and capacity preferences.
- If one pool becomes too expensive or unavailable, the fleet switches to another pool to maintain the requested capacity.
- This helps reduce costs while ensuring that enough instances are running for your workload.

### 34. Spot Request

- A Spot Request asks for a single Spot Instance, while a Spot Fleet requests and manages multiple Spot Instances across different instance types, AZs, and pricing strategies to optimize cost and availability.

### 35. EC2 burst balances

Think of EC2 burst balances as a savings account for CPU power. Your instance saves unused CPU capacity to spend later when it needs a performance boost.
EC2 burst balances represent the extra processing power your instance has stored up for short, high-performance tasks. They are like a "reserve energy tank" for your EC2 instance's CPU, allowing it to handle more work temporarily when needed.

### 36. Deployment Strategies

Immutable Updates: Deploys new instances in a separate group and switches traffic to them once they're ready. Old instances are terminated, leading to burst credit loss.
Traffic Splitting: Routes a percentage of traffic to newly created instances while gradually reducing traffic to the old ones. Once fully switched over, the old instances are terminated.
If you want to preserve EC2 burst balances during deployments, opt for in-place deployment strategies like Rolling or All-at-once, as they update existing instances without replacing them.

### 37. Understanding EC2 Burst Balance Loss During Deployments

If you notice EC2 burst balances are lost after a deployment, it is likely due to using immutable updates or traffic splitting mode, as these deployment strategies replace existing instances with new ones. To preserve burst balances, use in-place deployment strategies such as Rolling or All-at-once, which update existing instances instead of replacing them.

### 38. Burstable Instances

- Burstable Instances (T2/T3) run at normal CPU power but can boost performance ("burst") when needed using credits—if credits run out, the CPU slows down until new credits build up.
- T2/T3 instances are burstable EC2 instances that provide basic CPU performance but can temporarily boost ("burst") when needed using CPU credits, which accumulate when the instance is idle and deplete when high CPU power is used.
- As of now, T2, T3, T3a, and T4g are the only burstable instance types in AWS. These instances use CPU credits to provide temporary bursts of high performance when needed
- The Unlimited Mode for T2, T3, T3a, and T4g instances allows them to burst beyond their CPU credit limit without performance drops, but with additional charges if CPU usage stays high for too long.

### 39. Elastic IP

- An Elastic IP is a fixed public IPv4 address for an EC2 instance, which remains the same even if the instance stops and starts, can be remapped to another instance, and is free when attached but incurs charges when unused.

### 40. CloudWatch Metrics

- CloudWatch Metrics for EC2 include Basic Monitoring (free, 5-minute intervals) and Detailed Monitoring (paid, 1-minute intervals), tracking CPU, network, disk, and status checks.
- EC2 CloudWatch metrics include CPU usage, network traffic, status checks, and disk read/write, but do not track RAM usage, requiring custom metrics for memory monitoring.

### 41. EC2 CloudWatch metrics

- CPU Usage – Measures CPU utilization and burstable instance credit balance to track processing power.
- Network Traffic – Monitors incoming and outgoing network data to analyze bandwidth usage.
- Status Checks – Checks instance health, system hardware, and attached EBS volume status to detect issues.
- Disk Read/Write – Tracks read and write operations (Ops/Bytes) for instance store disks, but not for EBS volumes.

### 42. Custom CloudWatch metrics

- Custom CloudWatch metrics allow users to push their own data (e.g., RAM and application-level metrics) at 1-minute (basic) or 1-second (high) resolution, but require proper IAM permissions on the EC2 instance role.

### 43. Unified CloudWatch Agent

- By default, EC2 instances do not send logs to CloudWatch Logs, and you need the Unified CloudWatch Agent to collect and push system logs, as well as metrics like RAM and disk usage, which are not included in standard EC2 monitoring.
- It requires correct IAM permissions, supports centralized configuration via SSM Parameter Store, and uses CWAgent as the default namespace for collected metrics.
- The Unified CloudWatch Agent uses SSM Parameter Store to store and manage its settings in one place, so you can easily update and apply the same configuration to multiple instances.

### 44. Procstat plugin

- The procstat plugin in the Unified CloudWatch Agent monitors CPU, memory, and system utilization of individual processes on Linux and Windows. , and reports metrics with the "procstat" prefix (e.g., procstat_cpu_usage).
- Without the procstat plugin, you wouldn’t get process-level insights, making it harder to diagnose issues in complex applications.

### 45. Process-level insights

- Process-level insights provide detailed data on individual processes, including CPU and memory usage, uptime, resource spikes, and health status, helping identify and troubleshoot performance issues.
- EC2 standard monitoring in CloudWatch does not monitor process-level insights; it only tracks overall system metrics like CPU, disk, network, and instance status, but does not show which individual processes are using resources

### 46. AWS status checks

- AWS status checks help find and fix problems in EC2 instances by checking hardware, software settings, and storage health.
  - System Status Checks (SSC) → Check AWS hardware issues.
  - Instance Status Checks (ISC) → Check software and network issues inside the EC2 instance.
  - EBS Status Checks (ESC) → Check EBS volume (storage) health.

### 47. CloudWatch metrics and alarm

- CloudWatch metrics track EC2 instance health every minute, detecting system, instance, and EBS failures. If an issue is found, a CloudWatch Alarm can trigger recovery, keeping the same IP and settings while also sending notifications. Alternatively, an Auto Scaling Group can launch a new instance(set min/max/desired to 1), but it won’t retain the original IP.

### 48. Cloudwatch alarms

- CloudWatch Alarms can do more than just send notifications; they can trigger automated recovery actions for EC2 instances. If a system status check fails, CloudWatch can initiate EC2 instance recovery, which restarts the instance on new hardware while keeping the same private/public IP, EIP, and metadata. However, this works only for system failures and not for instance-level issues like OS crashes.

### 49. StatusCheckFailed_System

- CloudWatch monitors EC2 health using metrics like StatusCheckFailed_System and can trigger recovery actions. One option is using a CloudWatch Alarm to recover the instance with the same IP and metadata, while also sending notifications via SNS. Another option is using an Auto Scaling Group to launch a replacement instance, but it won’t retain the original private or Elastic IP.

### 50. EC2 statuscheck cloudwatch metrics

- Other Cloud watch metrics include StatusCheckFailed_System, StatusCheckFailed_Instance, StatusCheckFailed_AttachedEBS and StatusCheckFailed_Any.
- StatusCheckFailed_Any is a CloudWatch metric that triggers when either a system-level or instance-level status check fails. It combines both StatusCheckFailed_System (hardware or AWS infrastructure issues) and StatusCheckFailed_Instance (OS, network, or software issues), making it a useful metric for detecting overall instance health problems.

### 51. EC2 Hibernate

- EC2 Hibernate allows you to pause an instance while keeping its in-memory state, so when you start it again, everything resumes quickly without reloading the OS or applications, unlike stopping (which requires a full reboot) or terminating (which erases everything).

### 52. EC2 Image Builder

- EC2 Image Builder automates the creation, maintenance, validation, and testing of Amazon Machine Images (AMIs) for EC2 instances. It ensures AMIs are up-to-date, secure, and ready for deployment on a scheduled basis, reducing manual effort in managing VM or container images.
- EC2 Image Builder keeps AMIs up-to-date by automatically running scheduled builds that apply the latest updates, security patches, and configurations before validating and distributing the AMIs.

### 53. AMI

- AMIs are region-locked, meaning the same AMI ID cannot be used across different AWS regions.
- To migrate an EC2 instance to another Availability Zone (AZ), create an AMI from the existing instance and use it to launch a new EC2 instance in the desired AZ.

### 54. Sharing AMIs

- To allow another AWS account to use an AMI with an encrypted EBS snapshot, you must share the AWS KMS Customer Master Key (CMK) used for encryption with that account.

### 55. Using approved AMI

- To ensure that all EC2 instances are launched using the approved AMI, you can use AWS Config, which allows you to set compliance rules that monitor and enforce the use of the approved AMI.

### 56. AWS Systems Manager

- AWS Systems Manager helps you control and monitor your EC2 and on-premises servers, fix issues automatically, keep systems updated, and connect with CloudWatch and AWS Config for better tracking.

### 57. SSM agent

- AWS Systems Manager works by using the SSM agent, which must be installed on EC2 or on-premises servers (pre-installed on Amazon Linux 2 and some Ubuntu AMIs), and requires proper IAM permissions to function. If an instance cannot be controlled through SSM, the issue is likely due to a missing or misconfigured SSM agent or incorrect IAM role settings.

### 58. Resource Groups

- Resource Groups in AWS allow you to organize and manage multiple resources using tags, helping to group related resources such as applications, different layers of a stack, or production versus development environments, making it easier to manage infrastructure efficiently.

### 59. SSM Documents

- SSM Documents (SSM Docs) are JSON or YAML files that define automation actions, parameters, and commands for AWS Systems Manager, allowing users to execute predefined or custom tasks like software installation, patching, and configuration management on EC2 instances and other managed resources.
- In simple terms, they tell AWS what tasks to run on your servers, like installing software or updating settings, so you can manage them automatically.

### 60. SSM Automation

- SSM Automation helps automate common AWS tasks like restarting EC2 instances or creating backups using predefined or custom runbooks, which can be triggered manually, on a schedule, or by AWS services like EventBridge and AWS Config.

### 61. Runbooks

- Runbooks are a type of SSM Document specifically designed for automation tasks in AWS Systems Manager (SSM). They define the sequence of actions that should be executed on AWS resources, such as restarting an EC2 instance or taking an EBS snapshot.

### 62. SSM Parameter Store

- SSM Parameter Store securely stores configuration data and secrets, supports optional encryption with KMS, provides version tracking, integrates with AWS services like CloudFormation and EventBridge, and ensures access control through IAM.

### 63. SSM Parameter Store hierarchy

- The SSM Parameter Store hierarchy allows the organization of parameters into paths, such as by environment (e.g., dev, prod) or by department, and these parameters can be retrieved using APIs, like GetParameters or GetParametersByPath, to be used in different functions, such as Lambda functions for different environments.

### 64. Parameter tiers

- The Standard parameter tier in SSM Parameter Store is free and supports up to 10,000 parameters with a maximum size of 4 KB per parameter but does not allow parameter policies.
- The Advanced tier allows up to 100,000 parameters with a larger size limit of 8 KB per parameter and supports parameter policies but comes with a storage cost of $0.05 per parameter per month.
- The Advanced tier is useful when more storage, policies, and scalability are needed, while the Standard tier is best for basic, cost-free configurations.

### 65. Parameter policies

- Parameter policies in SSM Parameter Store allow you to set rules for advanced parameters, such as expiration to automatically delete parameters after a set time, expiration notifications to send alerts before parameters expire using EventBridge, and no change notifications to detect when parameters remain unchanged for a specified duration.

### 66. SSM Inventory

- SSM Inventory collects and stores metadata like software, updates, and configurations from EC2 or on-premises instances, allowing you to view, analyze, and query it across accounts and regions using tools like S3, Athena, or QuickSight.

### 67. SSM State Manager

- SSM State Manager helps you keep EC2 or on-premises instances in a specific, desired state by automatically applying settings like software installs or port rules on a schedule using SSM documents.
- In simpler terms help you make sure your EC2 or on-prem servers always stay the way you want—for example, with antivirus installed or certain ports closed—by checking and fixing them automatically on a set schedule.

### 68. SSM Patch Manager

- SSM Patch Manager helps you automatically update your EC2 and on-prem servers with the latest OS, application, and security patches, either on-demand or on a schedule using Maintenance Windows, and provides a report showing which patches are missing.

### 69. Maintenance Window

- A Maintenance Window in AWS is a scheduled time period when you allow AWS Systems Manager (SSM) to run tasks like patching, updates, or maintenance on your instances. It lets you control when these tasks happen so they don’t disrupt your system during busy hours.
- A Maintenance Window is a schedule you define to automatically run tasks like OS patching or software updates on selected instances during a specific time and duration.

### 70. Patch Baseline

- Patch Baseline is a list that controls which patches should or shouldn’t be installed on your ssm managed instances, and you can even auto-approve some patches after a few days.
- With Patch Baselines, you can automatically approve patches a certain number of days after they're released — for example, auto-approve all critical security updates 7 days after they become available. This helps keep your instances up to date without needing manual approval each time.

### 71. Patch Group

- A Patch Group is a tag that lets you link specific instances to a Patch Baseline, so you can apply different patch rules to different environments like dev or prod.
  -Patch Baselines define which patches should be installed, while Patch Groups define which set of instances those patches should be applied to.
- An instance can only be in one Patch Group.
- Patch Group can be registered with only one Patch Baseline.

### 72. Types of Patch Baselines

- Pre-Defined Patch Baselines are created and managed by AWS and cannot be changed, while Custom Patch Baselines are created by you, allowing full control over which patches to approve or reject and when to apply them.

### 73. AWS-RunPatchBaseline

- SSM Patch Manager works by tagging EC2 instances with a Patch Group (like "Dev" or "Prod"), which links them to a specific Patch Baseline that defines what patches to apply. When patching is triggered—either manually through the AWS Console or SDK, or on a schedule using Maintenance Windows—the AWS-RunPatchBaseline SSM document runs on the instances.

### 74. SSM Session Manager

- SSM Session Manager lets you securely access and control your EC2 or on-premises servers without needing SSH, bastion hosts, or SSH keys, and supports logging all sessions and commands for auditing.
- You can control who uses Session Manager and which EC2 instances they can access by using IAM permissions and instance tags, and you can also log session data to S3 or CloudWatch while restricting what commands users can run.
- SSH needs port 22 open and a public IP, which can be risky, while Session Manager connects securely without opening any ports, using IAM roles and logs session activity to CloudWatch or S3.

### 75. SSM Agent

- Make sure the SSM Agent is installed and running on EC2 instances for them to appear and be managed in AWS Systems Manager.
- To apply OS patches across all EC2 instances efficiently, the best approach is to use AWS Systems Manager (SSM).

### 76. SSM Patch Manager vs SSM Run Command

- The easiest way to patch a fleet of EC2 instances without using SSH is to use SSM Run Command, which lets you remotely execute commands on multiple instances at once.
- To automate patching of your managed instances, use SSM Patch Manager, which helps schedule and apply patches automatically across EC2 and on-premises systems.
- SSM Run Command is used for manually executing custom patch commands, while SSM Patch Manager automates and schedules OS patching with compliance tracking.

### 77. High availability vs scalability

- High availability ensures a system stays running without downtime, while scalability ensures a system can handle increased load by growing resources.
- Vertical scaling means changing the instance size (like upgrading to more RAM/CPU), horizontal scaling means adding more instances using Auto Scaling Groups and Load Balancers, and high availability means spreading those instances across multiple Availability Zones using the same tools.

### 78. When horizontal scaling turns into high availability

- Horizontal scaling becomes high availability when the instances are spread across multiple Availability Zones (AZs). So instead of just adding more instances in one AZ (which gives more capacity), you add them in different AZs, which protects your app from AZ failures—this way, it's both scalable and resilient.

### 79. Application Load Balancer

- An Application Load Balancer (ALB) is a service that automatically spreads incoming web traffic (HTTP or HTTPS) across multiple targets such as EC2 instances, containers, or IP addresses to ensure no single server is overwhelmed. It operates at Layer 7 (application layer) of the OSI model and supports advanced routing features like path-based routing, host-based routing, and redirect rules. ALB also improves application availability by routing traffic only to healthy targets in one or more Availability Zones and offers better support for microservices and container-based applications.

### 80. Network Load Balancer

- A Network Load Balancer (NLB) is designed to handle millions of requests per second with very low latency by operating at the connection level (Layer 4) of the OSI model. It routes traffic based on IP protocol data (TCP, UDP, or TLS), making it well-suited for high-performance applications like gaming, IoT, or financial systems. NLB preserves the source IP of the client, supports static IP addresses and Elastic IPs, and integrates with AWS PrivateLink. It is highly scalable, fault-tolerant, and can be used in both internet-facing and internal scenarios.

### 81. Gateway Load Balancer

- Gateway Load Balancer (GLB) does not load balance traffic directly to EC2 application instances like ALB or NLB do. It balances traffic across EC2 instances running security software, not EC2 instances running your application.
- GLB’s main job is to send traffic to third-party virtual appliances (like firewalls or intrusion detection systems), not to web servers or app servers.
- If you need:
  - Security filtering → Use GLB to send traffic through your security appliances.
  - Web/app traffic load balancing → Use ALB (for HTTP/HTTPS) or NLB (for TCP/UDP).
  - Both → You can chain them together: traffic goes through GLB (for inspection) → then to ALB/NLB (for routing to EC2).

### 82. GENEVE protocol

- The **GENEVE protocol** is a network tunneling protocol used by Gateway Load Balancer to **encapsulate and forward traffic** between virtual appliances while preserving original packet details like source IP.

### 83. Sticky sessions

- Sticky sessions use cookies to ensure a user's requests go to the same target, with either application-based or duration-based cookies depending on how the session is managed.
- Application-based cookies come from the app(EC2 Instances, i.e custom cookies) or load balancer to keep users on the same server, while duration-based cookies are made by the load balancer to stick users to one server for a set amount of time.
- For sticky sessions, ALB uses two types of cookies: Cookies(AWSALB, AWSALBAPP, AWSALBTG ) generated by the load balancer, and custom cookies generated by the application, allowing consistent routing of user requests to the same target.

### 84. Cross-zone load balancing

- With cross-zone load balancing, traffic is spread evenly across all targets in all Availability Zones, but without it, traffic is only spread within each zone's own targets, possibly leading to uneven load if zones have different numbers of instances.
- Application Load Balancer has cross-zone load balancing enabled by default with no extra cost, while Network and Gateway Load Balancers have it disabled by default and charge for inter-AZ traffic if enabled, and Classic Load Balancer also has it off by default but doesn’t charge if turned on.

### 85. SSL/TLS certificates

- SSL/TLS certificates encrypt data between your users and load balancer for secure communication, with TLS being the modern version of SSL, issued by trusted authorities, and requiring renewal after expiration.
- A load balancer uses a security certificate to safely connect users over HTTPS, and you can manage or upload this certificate using AWS tools like AWS Certificate Manager (ACM), with options to support many website names using one load balancer.

### 86. Uploading SSL/TLS certificates manually

- you can upload SSL/TLS certificates manually to the load balancer (instead of using ACM), by importing your own certificate files directly into AWS — this is useful if you're using certificates from external providers or self-signed ones.

### 87. Security certificate on HTTPS listeners

- A load balancer uses a security certificate on HTTPS listeners to safely connect users over HTTPS, ensuring encrypted communication between clients and the load balancer; this certificate can be managed or uploaded using AWS tools like AWS Certificate Manager (ACM), and a default certificate must be specified for the HTTPS listener to work properly.
  -The default certificate for the HTTPS listener is usually one of the security certificates managed by ACM. You can either create or import a certificate in ACM, and then select it as the default certificate for your HTTPS listener on the load balancer. So, ACM helps store and manage the certificate, while the HTTPS listener uses one of those certificates to handle secure (HTTPS) traffic.

### 88. SNI (Server Name Indication)

- SNI (Server Name Indication) lets a load balancer host multiple SSL certificates by allowing the client to tell the server which hostname it wants, so the correct certificate can be used for the connection.
- For example, If a user tries to visit www.mycorp.com, the client tells the load balancer this name during the SSL handshake, and the load balancer uses the matching SSL certificate for www.mycorp.com instead of a default one.
- SNI (Server Name Indication) helps by letting the client tell the load balancer which domain it wants to connect to during the SSL handshake, so the load balancer can choose and present the correct SSL certificate for that specific domain.

### 89. SNI supported Load Balancers

- Classic Load Balancer (CLB) only supports one SSL certificate, so if you want to host multiple secure websites (with different domain names), you need to create multiple CLBs;
- But with newer load balancers like the Application Load Balancer (ALB) and Network Load Balancer (NLB), you can use many SSL certificates at once by using SNI (Server Name Indication), which lets the load balancer choose the right certificate based on the hostname the client requests.

### 90. Connection Draining

- Connection Draining (or Deregistration Delay for ALB/NLB) lets in-progress requests finish on an EC2 instance that's being removed or is unhealthy, while stopping new traffic from going to it.
- For example, if an EC2 instance in a load balancer is being removed for maintenance, Connection Draining allows it to finish handling any active user requests before shutting down, so users don’t lose their session or get errors.

### 91. ELB Health Checks

- ELB Health Checks automatically monitor the status of each target (like EC2 instances) by regularly sending requests to a specific path and port, marking targets as healthy or unhealthy based on response success or failure, and only routing traffic to healthy targets unless none are available.

![ELB Health Checks](./img/ELB%20Health%20Checks.png)

### 92. Unhealthy target group

- If all targets in a target group are marked unhealthy, the Elastic Load Balancer (ELB) will still route requests to them, assuming that the health checks might be incorrect or too strict. This helps avoid total downtime, giving your app a chance to still respond, even if it’s struggling.

### 93. Common load balancer error codes

- 200 means success, 3xx means redirected requests, 4xx codes (like 400, 401, 403, 460, 463) mean there was a problem from the client side, and 5xx codes (like 500, 502, 503, 504, 561) mean there was a problem from the server or ELB itself.

### 94. Load balancer metrics

- All load balancer metrics are automatically sent to CloudWatch, allowing you to monitor key performance indicators such as the number of healthy or unhealthy targets, request counts, latency, and HTTP error codes.
- Metrics like SurgeQueueLength show how many requests are waiting to be routed due to busy targets, and SpilloverCount tracks how many were dropped because the queue was full. These insights help you detect problems, understand traffic patterns, and trigger auto scaling to maintain performance and availability.

### 95. Troubleshoot load balancer

- To troubleshoot load balancer issues using metrics, you can look at HTTP error codes: a 400 error means the client sent a bad request, a 503 indicates no healthy instances are available in the configured Availability Zones (check HealthyHostCount in CloudWatch).
- 504 means the connection timed out, possibly due to mismatched keep-alive settings between the EC2 instance and the load balancer's timeout settings.
- A mismatched keep-alive setting happens when your EC2 instance has a keep-alive timeout of 1 minute but the load balancer has an idle timeout of 30 seconds, causing the connection to close too early. It’s like the EC2 server is waiting 1 minute to hear back, but the load balancer gives up after 30 seconds—so the connection gets cut off too soon.

### 96. Load Balancer access logs

- Load Balancer access logs store important request details in S3 like IP address and response time, helping with troubleshooting and compliance, and are already encrypted—you only pay for the S3 storage.

### 97. Application Load Balancer Request Tracing

- Application Load Balancer adds a special header called X-Amzn-Trace-Id to each request to help track it in logs or tracing tools, although it's not yet integrated with AWS X-Ray.

### 98. ALB Target group settings

- Target group settings let you control how requests are routed (like round robin or least requests), how long to wait before removing a target, and how sticky sessions work using application-based or duration-based cookies.

### 99. ALB Target group Slow Start Mode

- Slow Start Mode gradually increases the number of requests sent to a new or recovering target, giving it time to warm up before handling its full share of traffic. To disable slow start, set it's duration value to 0.

### 100. Load Balancers Request Algorithms

- The Least Outstanding Requests algorithm sends traffic to the EC2 instance with the fewest active or pending requests, helping balance load more efficiently.
- The Round Robin routing algorithm sends each new request to the next EC2 instance in line (even if they dont have the fewest active or pending requests), evenly distributing traffic across all available targets.
- The Flow Hash algorithm, used by Network Load Balancer, picks a target EC2 instance by creating a unique hash from connection details like IPs and ports, and keeps sending all traffic from that connection to the same instance.

### 101. ALB listeners

- ALB listeners are like traffic guards that listen for web requests on a specific port (like 80 or 443) and decide where to send the request based on rules you set.
- ALB listener rules are checked in order and use conditions like host, path, or method to decide whether to forward, redirect, or return a fixed response to a specific target group.

### 102. Target Group Weighting

- Target Group Weighting lets you split traffic between different target groups (like app versions) by assigning percentages using weights in one listener rule.

### 103. Auto Scaling Group

- An Auto Scaling Group automatically adds or removes EC2 instances based on demand to keep your application available and cost-efficient.
- An Auto Scaling Group uses a launch template with settings like AMI, instance type, and network details, along with size limits and scaling rules, to automatically manage EC2 instance numbers.

### 103b. Launch Templates vs Launch Configurations

- Launch Templates are the newer(the older one is Launch template), recommended option over Launch Configurations, offering versioning, advanced features, and more flexibility for launching EC2 instances.
- They must be recreated each time a change is needed, as they cannot be edited.
- Auto Scaling Groups can automatically increase or decrease the number of EC2 instances based on CloudWatch alarms that track metrics like average CPU usage.

### 104. Scaling Policies

- Auto Scaling Groups support **Dynamic Scaling** (reacts to metrics like CPU), **Simple/Step Scaling** (adds/removes units based on CloudWatch alarms), and **Scheduled Scaling** (scales at specific times based on usage patterns).
- An example of Simple/Step Scaling is: when CPU usage goes above 70%, add 2 EC2 instances, and when it drops below 30%, remove 1 instance.
- An example of Dynamic Scaling is: automatically add or remove instances to keep the average CPU usage of the Auto Scaling Group around 40%.
- Predictive scaling analyzes past usage patterns, forecasts future demand, and automatically schedules scaling actions ahead of time.

### 105. Metrics to scale on

- Good metrics to scale on include CPUUtilization, RequestCountPerTarget, Network In/Out, and any custom CloudWatch metric based on your application's behavior.

### 106. ASG Cooldown period

- After a scaling action, Auto Scaling Groups enter a cooldown period (default 300 seconds) to prevent launching or terminating more instances until metrics stabilize.

### 107. Auto Scaling Group lifecycle hooks

- Auto Scaling Group lifecycle hooks let you run custom actions before an instance launches or terminates, useful for setup, cleanup, or integration with services like Lambda, SNS, SQS, or EventBridge.

### 108. Pending stage

- The pending stage is when an EC2 instance is launching but not yet in service, allowing you to run setup actions before it becomes active.
  When an instance enters the Pending or Terminating state, a lifecycle hook can put it into a Wait state, allowing you to run scripts, extract logs, perform cleanup, or carry out health checks.
  ![ASG Lifecycle hooks](./img/ASG_Lifecycle_hooks.png)

### 109. Troubleshooting Auto Scaling Group (ASG)

- To troubleshoot Auto Scaling Group (ASG) issues, check if the maximum capacity limit is reached, verify that the security group and key pair exist, and note that if ASG fails to launch instances for over 24 hours, it will automatically suspend scaling processes.

### 110. ASG CloudWatch collects ASG-level metrics

- CloudWatch collects ASG-level metrics every minute (if enabled) like group size and instance states, while EC2-level metrics such as CPU utilization are available by default with basic (5-minute) or detailed (1-minute) monitoring.

### 111. Elastic Beanstalk

- Elastic Beanstalk is a free service that helps developers put their apps online easily by handling things like servers, scaling, and health checks for them, while they only need to take care of their code and pay for the servers used.
- Elastic Beanstalk has three main parts: Application (a group of environments and versions), Application Version (a specific upload of your app code), and Environment (the AWS setup that runs one version, like dev, test, or prod), and you follow steps like creating the app, uploading a version, launching an environment, and managing it.

### 112. Elastic Beanstalk Tiers

- In Elastic Beanstalk, the **Web Server Tier** handles real-time user requests through a load balancer and web servers, while the **Worker Tier** processes background tasks by pulling messages from an SQS queue and scaling based on the number of messages.
- Typically, the Web Server Tier handles front-end tasks like user requests, and it can send background jobs to the Worker Tier using an SQS queue. The Worker Tier then pulls those jobs and processes them in the background (e.g., image processing, sending emails).
- So, you can use both tiers in one application when you want to separate real-time tasks from background processing.

### 113. Elastic Beanstalk deployment modes

- Elastic Beanstalk has two deployment modes: **Single Instance**, best for development with one EC2 and RDS in one zone, and **High Availability**, best for production with multiple EC2s in an Auto Scaling Group across zones behind a Load Balancer, plus RDS with a standby for failover.

### 114. AWS CloudFormation

- AWS CloudFormation lets you define your cloud setup (like EC2, S3, load balancers) in a file, and then it automatically creates everything for you in the correct order with the exact settings you wrote.
- AWS CloudFormation is great because it lets you manage your infrastructure as code (no manual setup, supports version control, and easier reviews), and it helps save costs by tagging resources, estimating prices, and automating creation and deletion for things like dev environments.

### 115. CloudFormation updates

- CloudFormation works by uploading your template to S3, referencing it in CloudFormation to create a named stack, and any updates require a new template upload—deleting the stack removes all the created resources.

### 116. CloudFormation deployment and components

- You can deploy CloudFormation templates either manually using the console and editors for learning or visually designing, or automatically using YAML files with AWS CLI or CI/CD tools for full automation.
- A CloudFormation template is made up of components like version info, description, required resources, parameters, mappings, outputs, and conditions, with helpers like references and functions to make the setup flexible and reusable.

### 117. CloudFormation Resources

- **Resources** are the most important part of a CloudFormation template because they define which AWS components will be created, and AWS automatically handles their setup, updates, and deletion using resource identifiers like `service-provider::service-name::data-type-name`. For example, AWS::EC2::Instance.
- Resources are the only strictly mandatory component in a CloudFormation template. Only Resources must be present for the template to work.

### 118. CloudFormation Parameters

- Parameters in CloudFormation are input values you give when launching a stack, allowing you to reuse templates with different settings and making your templates flexible and protected from errors through typed inputs.
- You should use a parameter when a CloudFormation setting might change in the future, so you can just pass in a new value without needing to re-upload or edit the whole template.
- You pass parameter values when you create or update a stack — not while the stack is actively running. Once the stack is created, the parameters are locked in until you manually update the stack.

### 119. Example of Parameters

```Parameters:
  InstanceTypeParameter:
    Description: EC2 instance type
    Type: String
    Default: t2.micro
    AllowedValues:
      - t2.micro
      - t2.small
      - t2.medium
    ConstraintDescription: must be a valid EC2 instance type.
```

- InstanceTypeParameter is the name of the parameter.
- When you create the stack, you can choose the instance type.
- If you don't choose, it defaults to t2.micro.
- It only accepts the values listed in AllowedValues, and shows a message if it's invalid
- CloudFormation parameters uses "NoEcho: true" to hide the password value from logs or output, which is useful for secrets.

### 120. Parameter referencing

- Parameter referencing means using `!Ref` (short for Fn::Ref) to point to a parameter, which pulls in the value you gave during stack creation and uses it in the right place in the template.

### 121. Pseudo Parameters

- Pseudo Parameters are built-in values that AWS CloudFormation makes available automatically in any template, like your AWS account ID or region, and you can use them without declaring them yourself.
- They're called "pseudo parameters" because they act like normal parameters (you can reference them in your template), but you don’t need to define or pass them in—CloudFormation automatically provides their values for you based on the environment.

### 122. CloudFormation Mappings

- Mappings in CloudFormation are like fixed lookup tables inside your template that help you pick values based on things like region or environment, and they are hardcoded, meaning you don’t pass them in during stack creation.
- You can use a mapping to select instance types for different environments, like dev gets t2.micro and prod gets m5.large. Then you use Fn::FindInMap to pull the correct instance type based on the environment name.

### 123. Using !FindInMap for Region-Specific AMIs in CloudFormation

- In a CloudFormation template, !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] is used to retrieve values from a map based on the region or other environment-specific settings. This approach allows you to dynamically select configuration values based on deployment regions, making templates flexible for multi-region deployments. e.e - ImageId: !FindInMap [ RegionMap, !Ref "AWS::Region", AMI ]
- For example, you can use a mapping to choose the right AMI ID based on the AWS region — like using us-east-1 to return ami-0ff8a91507f77f867. In your template, you’d use the Fn::FindInMap function to get the right value based on the region.
- ImageId: !FindInMap [ RegionMap, !Ref "AWS::Region", AMI ]

### 124. When to use Mappings and Parameters

- Use mappings when values depend on known factors like region or environment, and use parameters when values are specific to the user and need to be input manually.

### 125. CloudFormation outputs

- CloudFormation outputs let you optionally share values like VPC or Subnet IDs from one stack to another, making it easier for different teams to collaborate and reuse resources. It exports values like VPC ID or Subnet ID from one stack so that other stacks can import and use them.
- You can import a value (like a security group ID) exported by another stack, using !ImportValue.
- You can't delete the original stack until that reference is removed.

### 126. CloudFormation Conditions

- Conditions in CloudFormation let you create or skip resources based on specific values like environment type or region, so the same template can behave differently for dev, test, or prod setups.
- The condition CreateProdResources: !Equals [ !Ref EnvType, prod ] means "only do something if the parameter EnvType is set to prod during stack creation."
- ````Resources:
  MountPoint:
    Type: AWS::EC2::VolumeAttachment
    Condition: CreateProdResources```
  ````
- This means the `MountPoint` resource will only be created if the `CreateProdResources` condition is true during stack creation.

### 127. Difference between !Ref and !GetAtt

- Fn::Ref function (or !Ref in YAML) is used in CloudFormation to fetch the value of a parameter given during stack creation or the physical ID of a resource, allowing that value to be reused in the template.
- The Fn::GetAtt function (or !GetAtt in YAML) is used in CloudFormation to fetch specific attributes of a resource, like getting the Availability Zone of an EC2 instance.
- The difference is that !Ref returns the value of a parameter or the physical ID of a resource, while !GetAtt returns a specific attribute (like Availability Zone or DNS name) of a resource.

### 128. CloudFormation intrinsic function condition

- You can define a condition in AWS CloudFormation using intrinsic functions like Fn::Equals, Fn::And, Fn::If, Fn::Not, and Fn::Or.
- A condition is given a logical name like CreateProdResources, and it checks if a value (like the environment type) matches something specific (like prod).
- If the condition is true, the related resources or outputs will be created. This helps you control what gets deployed depending on parameters or other logic in your template.

### 129. CloudFormation rollback

- In AWS CloudFormation, if a stack creation fails, all resources are deleted by default as part of a rollback, though you can disable this rollback to troubleshoot the issue.
- When a stack update fails, CloudFormation automatically rolls back to the last known good state, and logs are available to help identify what went wrong.
- If the rollback itself fails, you need to manually fix the problem and then resume the rollback process using the `ContinueUpdateRollback` API through the console or CLI.

### 130. CloudFormation service role

- A CloudFormation service role is an IAM role that lets CloudFormation create, update, or delete resources in a stack on your behalf, even if the user launching the stack doesn’t have direct permissions to those resources.
- This is useful when you want to follow the principle of least privilege—allowing users to deploy infrastructure without giving them full access to every resource. To make this work, the user only needs CloudFormation permissions and iam:PassRole to pass the service role to CloudFormation.

### 131. CloudFormation capabilities

- CloudFormation capabilities are like special permissions you must allow before CloudFormation can do certain actions.
- If your template creates IAM users or roles, you need to allow CAPABILITY_NAMED_IAM. If your template changes itself using macros or has stacks inside stacks, you need to allow CAPABILITY_AUTO_EXPAND.
- If you forget to give these permissions, CloudFormation will stop and give you an error to keep your account safe.

### 132. Macros

- Macros in AWS CloudFormation are special functions that let you transform or customize your template before it gets processed. They allow you to write reusable logic using AWS Lambda so that you don’t have to repeat code in your templates.
- For example, you can create a macro to automatically expand a short keyword into a full block of CloudFormation code. This helps make templates shorter, easier to manage, and more powerful.

### 133. Simple example usage of CloudFormation capabilities

```
aws cloudformation create-stack \
  --stack-name myStack \
  --template-body file://template.yaml \
  --capabilities CAPABILITY_NAMED_IAM
```

- If your template creates IAM resources like a role or user, you must specify --capabilities CAPABILITY_NAMED_IAM, otherwise CloudFormation will throw an error to protect you from creating IAM resources without realizing it.

### 134. CloudFormation DeletionPolicy

- DeletionPolicy in CloudFormation lets you control what happens to a resource when the stack is deleted—by default, it's set to Delete, meaning the resource is removed too, but it won’t work for S3 buckets if they’re not empty.
- DeletionPolicy: Retain on a resource tells CloudFormation to keep that resource (like a DynamoDB table) even if the stack is deleted, which helps preserve important data or configurations.

### 135. CloudFormation DeletionPolicy II

- The Snapshot deletion policy creates a final backup of supported resources like RDS or EBS before deleting them, so you can recover the data later if needed.
- If you set DeletionPolicy: Delete on an S3 bucket in your CloudFormation template, CloudFormation will try to delete the bucket when the stack is deleted—but if the bucket still has files inside, the deletion will fail because S3 doesn't allow deleting non-empty buckets.

### 136. CloudFormation Stack policies

- Stack policies in CloudFormation are JSON rules that control which resources can or cannot be updated during stack updates, helping prevent accidental changes—like allowing updates to everything except a production database.

### 137. Termination Protection

- Termination Protection in CloudFormation helps prevent accidental deletion of stacks by requiring you to disable it first before deleting the stack.

### 138. Custom Resources

- Custom Resources in CloudFormation let you create or manage resources not supported by default(by cloudformation) by using Lambda functions or SNS topics to run custom logic during stack operations.
- To delete a non-empty S3 bucket in CloudFormation, you can use a custom resource with a Lambda function to empty the bucket before deletion.

### 139. Defining a custom resource in CloudFormation

- To define a custom resource in CloudFormation, you use a ServiceToken to tell CloudFormation where to send the request — for example, it can send the request to a Lambda function's ARN. You can also pass extra input parameters if needed.
- A simple example of using a ServiceToken is telling CloudFormation to send the custom resource request to a specific Lambda function, like this:

```
ServiceToken: arn:aws:lambda:us-east-1:123456789012:function:MyCleanupFunction
```

- This tells CloudFormation to call that Lambda function to handle the custom task.

### 140. Dynamic references

- CloudFormation dynamic references let you securely pull sensitive or external values (like passwords or configs) from Systems Manager Parameter Store or Secrets Manager during stack operations.

### 141. Fn::Base64

- You can run EC2 user data scripts in CloudFormation by passing the script using the Fn::Base64 function, and the output can be checked in /var/log/cloud-init-output.log.
- In CloudFormation, `Fn::Base64` is used because EC2 expects the **user data** to be **Base64-encoded**, so this function automatically converts your script into the format EC2 can understand when launching the instance.

### 142. Cloud-init

- Cloud-init is a special tool built into Amazon EC2 (and other cloud systems) that runs when a new instance starts. It handles initial setup tasks, like installing software, setting hostnames, adding users, or running your user data script, making sure your instance is ready to use just after launch.
- The name cloud-init-output.log is used because it's the output log file created by the cloud-init tool, which runs during EC2 instance launch. It stores all the messages and results from the user data script and other initialization steps, so you can check what cloud-init did and if anything went wrong.

### 143. Helper Scripts

- EC2 user data scripts have limitations like difficulty in handling large setups, updates, readability, and success tracking—so CloudFormation helper scripts like `cfn-init` and `cfn-signal` are used to solve these problems.

### 144. cfn-init

- AWS::CloudFormation::Init lets you define EC2 setup steps in order—like installing packages, creating users, adding files, and starting services—using the config section inside the Metadata block of the Resources section
- The cfn-init script runs on an EC2 instance to read and apply setup instructions from the AWS::CloudFormation::Init metadata, like installing packages and starting services, and logs actions to /var/log/cfn-init.log.

### 145. Difference between AWS::CloudFormation::Init and cfn-init

- **AWS::CloudFormation::Init** is used in the template to define what should be done on the EC2 instance (like install packages, create files, run commands, etc.).
- **cfn-init** is a script that runs inside the EC2 instance to read and apply what you defined in AWS::CloudFormation::Init.

### 146. cfn-signal

- To let CloudFormation know that an EC2 instance is fully configured after running `cfn-init`, we use the `cfn-signal` script along with a `WaitCondition` and a `CreationPolicy` to pause the stack until a success or failure signal is received.
- A WaitCondition is a CloudFormation resource that pauses stack creation until it receives a success or failure signal, while a CreationPolicy is used to tell CloudFormation to wait for that signal (like from cfn-signal) before marking the resource (like EC2) as created—together they help make sure setup scripts finish before moving on.

### 147. cfn-signal failure

-If a wait condition doesn’t get a signal from the EC2 instance, make sure CloudFormation helper scripts like `cfn-init` and `cfn-signal` ran successfully, check logs (`/var/log/cloud-init.log`), ensure internet access, and optionally disable rollback to debug the instance.

- Disabling rollback lets you keep the failed EC2 instance running after a stack failure, so you can log in, check logs (like `/var/log/cloud-init.log`), and figure out what went wrong instead of CloudFormation deleting it automatically.

### 148. Nested stacks vs Cross stacks

- Nested stacks in CloudFormation let you break your templates into reusable parts by placing stacks inside other stacks, making updates easier and following best practices.
- Cross stacks share resources between stacks using `Export` and `ImportValue`, while nested stacks organize and reuse components within a parent stack but are not shared across other stacks.

### 149. DependsOn

- DependsOn in CloudFormation ensures one resource (like an EC2 instance) is created only after another specified resource (like a database) is successfully created.

### 150. StackSets

- CloudFormation StackSets let you deploy, update, or delete stacks across multiple AWS accounts and regions from a single administrator account using one template.
- CloudFormation StackSets offer two permission models: self-managed (you manually create IAM roles in all accounts) and service-managed(uses AWS Organization which automatiically handles permissions).

### 151. Service-managed StackSets

- Service-managed StackSets means AWS does the hard work for you—it automatically creates the needed IAM roles and pushes your CloudFormation stacks to all accounts in your AWS Organization, as long as you enable trusted access.
- StackSets with AWS Organizations lets you automatically deploy stacks to new accounts, delegate admin rights, and requires trusted access to be enabled first.

### 152. CloudFormation troubleshooting

- When troubleshooting CloudFormation, DELETE_FAILED happens when resources like S3 buckets are not empty or when security groups still have active EC2 instances; you can solve this by using Lambda-backed custom resources or setting DeletionPolicy=Retain.
- UPDATE_ROLLBACK_FAILED can occur due to external changes to resources, missing permissions, or Auto Scaling Groups not receiving enough signals—this requires manually fixing the issue and then running ContinueUpdateRollback to recover the stack.

### 153 CreationPolicy ResourceSignal count

- A CreationPolicy in CloudFormation is used to tell AWS to wait for a certain number of success signals before marking a resource (like an EC2 instance or Auto Scaling Group) as fully created.
- The ResourceSignal count is the exact number of signals (using cfn-signal) that must be received within the timeout period—if not enough signals come in, the resource creation fails and CloudFormation rolls back.

### 154. Cloudformation - Auto Scaling Groups sinal issue

- In CloudFormation, when you use Auto Scaling Groups (ASGs) with a CreationPolicy, you often set a ResourceSignal count to make sure all EC2 instances in the group are launched and configured properly.
- If the expected number of signals (like from cfn-signal) is not received within the timeout, CloudFormation thinks the setup failed, so it rolls back the whole update.

### 155. Continue from 154

- This usually means the instances didn’t run cfn-signal correctly, maybe due to missing scripts, permission errors, or network issues.
- if the Auto Scaling Group doesn’t receive enough signals, you need to manually investigate and fix the issue (like checking cfn-signal usage, instance logs, or network issues), then run ContinueUpdateRollback to recover the stack.

### 156. OUTDATED StackSets

- In StackSets, OUTDATED usually means the operation failed — the stack instance couldn't update as expected, so it no longer matches the current version of the StackSet. You’ll need to check what went wrong (like permissions or limits) and re-deploy to bring it up to date.
- If a StackSet shows an OUTDATED status, it likely failed due to missing permissions, non-unique global resources like S3 buckets, lack of trust between admin and target accounts, or hitting resource limits in the target account.

### 156. Lambda

- AWS Lambda runs short, on-demand functions with no server management and automatic scaling, unlike EC2 which requires managing virtual servers and scaling manually.
- AWS Lambda is cost-effective, easy to scale, supports many languages, integrates well with AWS services, and improves performance as you increase the Lambda RAM.

### 157. Lambda supported languages

- AWS Lambda supports popular languages like **Node.js**, **Python**, **Java**, C# (.NET Core), PowerShell, Ruby, and even custom runtimes like Rust or Go using the Runtime API, plus you can use container images that follow Lambda’s Runtime API rules.

### 158. Example of lambda usage

- When a new image is uploaded to S3, it can be configured to trigger a Lambda function that creates a smaller thumbnail image (also saved to S3) and stores details like image name and size in DynamoDB.
- This is a good example of Lambda usage because it shows how Lambda runs automatically (on-demand) when a new image is uploaded to S3 — there’s no need to manage any servers. Lambda does a quick job (resizing the image), then stops running, saving cost and scaling easily if many images are uploaded.

### 159. Another example

- AWS Lambda can also be used with EventBridge to run a scheduled task automatically every hour, without needing to manage any servers.
- AWS Lambda functions can be triggered automatically by EventBridge rules, either on a set schedule (like every hour using CRON) or when a CodePipeline changes state, allowing automated tasks to run without manual action.

### 160. S3 versioning and Notification

- Enabling versioning on an S3 bucket ensures that every write creates a new version of the object, which helps guarantee that a separate event notification is sent for each write—even if multiple writes happen at the same time(which can omit notifications sometime if versioning is not enabled).

### 161. A Lambda Execution Role

- A Lambda Execution Role (IAM role) gives Lambda functions permission to access AWS services like CloudWatch, SQS, and DynamoDB, and it's best practice to use one unique role per function.
- This role is essential because Lambda itself doesn’t have permissions by default—it assumes the role at runtime to perform tasks. You define what actions are allowed using IAM policies attached to this role.

### 162. Lambda resource-based policies

- Lambda resource-based policies let you give other AWS accounts or services permission to use your Lambda function, just like S3 bucket policies, allowing access through either IAM policies or the resource-based policy itself.

### 163. Lambda and CloudWatch Logs

- AWS Lambda uses CloudWatch Logs to store function output and CloudWatch Metrics to track performance like invocations, errors, duration, and throttling, but it needs an IAM role with logging permissions to do this.

### 164. AWS Lambda tracing

- AWS Lambda tracing is a feature that helps you see what your Lambda function is doing step by step, by recording its behavior and performance using AWS X-Ray. It lets you trace how the function runs, how long each step takes, where errors happen, and how it interacts with other AWS services—making it easier to debug issues and monitor performance.

### 165. Enabling AWS Lambda tracing

- To enable AWS Lambda tracing with X-Ray, you must activate Active Tracing in the Lambda settings, use the AWS X-Ray SDK, ensure the correct IAM role (AWSXRayDaemonWriteAccess) is attached, and Lambda will automatically run the X-Ray daemon and use environment variables to send trace data.

### 166. Lambda RAM and VCPU

- You can give your Lambda function more memory (RAM), and when you do, it also gets more CPU power(i.e vcpu) to run faster, especially for heavy tasks, and you can make it run up to 15 minutes if needed.

### 167. Lambda execution context

- The Lambda execution context is a temporary environment that keeps things like database or HTTP connections ready, so the next function run can be faster by reusing them instead of starting from scratch.
- To make Lambda faster, it's better to initialize things like database connections outside the function handler so they can be reused across multiple calls instead of re-created every time.

### 168. Lambda `/tmp` directory

- Lambda functions can use the `/tmp` directory (up to 10GB) for temporary storage across invocations, but for long-term or encrypted storage, use S3 with KMS data keys.

### 169. Lambda concurrent executions

- **Reserved concurrency** means setting aside a specific number of Lambda executions _just_ for one function, so it always has those available, and other functions can't use them—even if they’re idle. This guarantees that function will run when needed and won’t be throttled by other functions.
- Lambda Reserved Concurrency is a setting that guarantees a specific number of concurrent executions are always available just for that function, and also sets a hard limit so the function can’t use more than that number at the same time. This protects it from being throttled and also prevents it from using up all the concurrency and affecting other functions.

### 170. Reserved concurrency

- AWS Lambda supports up to 1000 concurrent executions, can reserve concurrency per function, and throttles extra requests—synchronously returning a 429 error or asynchronously retrying and sending to a DLQ. This means Lambda can run up to 1000 functions at the same time, and if more come in, it either shows an error or tries again later depending on how it's called.

### 171. AWS account concurrency limit

- By default, all Lambda functions in an AWS account share the same concurrency limit, which is 1,000 concurrent executions per Region (you can request a higher limit). If one function uses up too much concurrency, other functions may get throttled.
- Reserved concurrency solves this by carving out part of the 1,000 just for one function, so it's protected and guaranteed to run even if others are busy.

### 172. Provisioned concurrency

- Cold starts in Lambda cause delay when new instances load code for the first time, but provisioned concurrency avoids this by keeping functions pre-initialized for faster response.
- Reserved concurrency sets a limit on how many instances a specific Lambda function can use, while provisioned concurrency keeps a set number of instances pre-initialized to avoid cold starts and ensure low-latency execution.

### 173. Lambda CloudWatch metrics and Alarms

- AWS Lambda CloudWatch metrics help monitor function performance by tracking invocations, execution time, errors, throttles due to concurrency limits, failed DLQ deliveries, stream processing delays, and the number of functions running at the same time.
- CloudWatch Alarms for Lambda can alert you when no invocations happen for a set time, when there are function errors, or when throttling occurs due to concurrency limits.

### 174. Lambda CloudWatch Logs

- To send logs to CloudWatch, a Lambda function needs an execution role with permissions to create log groups and streams, and to write log events.
- CloudWatch Logs Insights lets you search and analyze your Lambda function logs, such as finding how many errors occurred in the last 7 days.
- CloudWatch Logs Insights provides queries to help you analyze Lambda logs for errors, cold starts, memory usage, average duration, and timeouts.

### 175. Lambda Insights

- Lambda Insights collects and summarizes system metrics and diagnostics (like CPU, memory, cold starts, and timeouts) to help you quickly find and fix issues with your Lambda functions using a special CloudWatch extension.
- Lambda Insights shows how your function is running (like CPU, memory, and cold starts), while CloudWatch Log Insights helps you search and understand what's inside your function’s logs.

### 176. EBS Volume

- An EBS Volume is like a network USB stick that stores data for EC2 instances, can only connect to one instance at a time in one availability zone, and keeps data even after the instance is stopped.
- Multi-Attach EBS volumes are a special type of EBS volume that can be attached to multiple EC2 instances at the same time, allowing them to all read and write to the same volume, but they must be in the same Availability Zone and use the io1 or io2 volume type.
- EBS snapshots are **backups of your EBS volumes** that are stored in **Amazon S3**, and they allow you to **restore your volume data at any time**, even in a **different Availability Zone or region**.

### 177. Delete on termination

- **Delete on termination** is a setting for EBS volumes that controls whether the volume is automatically deleted when the EC2 instance it's attached to is terminated; if set to `true`, the volume is deleted with the instance, and if set to `false`, the volume is kept and must be deleted manually.

### 178. Instance Store

- EC2 Instance Store is a temporary, high-performance storage physically attached to the host, ideal for fast I/O and temporary data, but all data is lost if the instance stops, so backups are your responsibility. In other words, EC2 Instance Store is fast storage built into the machine, good for temporary files, but you’ll lose everything if the instance stops, so you need to back it up yourself.

### 179. EBS volumes types

- EBS (Elastic Block Store) volumes come in six types, including SSD options like gp2/gp3 for general use and io1/io2 for high-performance needs, and HDD options like st1 and sc1 for large, infrequently accessed data
- Only gp2/gp3 and io1/io2 types can be used to boot EC2 instances.

### 180. General Purpose SSD

- General Purpose SSD (gp2 and gp3) are cheap and fast storage types used for things like starting your system or testing code; gp3 gives you steady speed and can go much faster if needed(a baseline of 3,000 IOPS with the ability to scale up to 16,000 IOPS and 1,000 MiB/s), while gp2 depends on the size(links IOPS to volume size)—bigger size means better speed, and small ones can speed up for short time.

### 181. Provisioned IOPS

- Provisioned IOPS (PIOPS) SSD volumes like io1 and io2 are used for important business apps that need fast, steady storage performance—especially databases; io1 supports up to 64,000 IOPS (32,000 for non-Nitro EC2) and lets you set performance separately from storage size, while io2 Block Express goes even faster with up to 256,000 IOPS, super low delays, and supports attaching to many EC2s at once (multi-attach).

### 180. EBS Multi-Attach

- EBS Multi-Attach lets you connect one io1/io2 volume to up to 16 EC2 instances **in the same Availability Zone**, with full read/write access, but it requires cluster-aware file systems to safely handle shared writes.

### 181. EBS Resizing

- You can only increase the size or IOPS (for io1) of an EBS volume—not decrease it—and after resizing, you must repartition(you need to adjust the space on the disk) the drive, though the volume stays usable even during the long optimization phase.
- After you increase the size or performance of an EBS volume, AWS takes some time in the background to fully apply and optimize the changes (like spreading data across more hardware for better speed). During this time, the volume still works normally, but you might not immediately get the full performance improvement until optimization finishes.

### 182. EBS snapshot

- An **EBS snapshot** is like taking a photo of your disk (EBS volume) at a moment in time so you can back it up and restore it later, and you can even copy it to other availability zones or regions without needing to stop the instance.

### 183. Amazon Data Lifecycle Manager

- Amazon Data Lifecycle Manager (DLM) helps you **automatically create, keep, and delete EBS snapshots and AMIs** on a schedule, using **tags** to know which resources to manage, but it **only works with snapshots and AMIs created by DLM**, not with ones made manually or from instance store.

### 184. Fast Snapshot Restore(very expensive)

- Fast Snapshot Restore (FSR) makes EBS volumes created from snapshots ready to use right away without waiting for each block to load, by fully preparing the data in advance, but it costs extra and is set for each Availability Zone.
- "Waiting for each block to load" means that when you create a volume from a regular snapshot, the data isn't fully ready yet—each piece (or block) of data has to be fetched from Amazon S3 the first time you try to use it, which causes delays (latency). This means the volume works, but the first time you access parts of it, it will be slow until everything is loaded in.

## 185. EBS delay explained

- When you restore an EBS volume from a snapshot, the snapshot is stored in Amazon S3. So the first time you try to use the volume, the data is not yet fully copied to the volume. Instead, each piece (called a _block_) is fetched from S3 only when it's needed.
- This saves time during creation but causes a small delay the first time you read or write each block. Once a block is fetched and used, it's kept locally, so the delay doesn’t happen again for that block.

### 186. EBS Snapshot Archive

- EBS Snapshot Archive allows you to move snapshots to a low-cost archive tier that is up to 75% cheaper, though it takes 24 to 72 hours to restore.
- The Recycle Bin for EBS Snapshots enables recovery of deleted snapshots by retaining them for a user-defined retention period ranging from 1 day to 1 year, helping protect against accidental deletions.

### 187. Amazon EFS

- Amazon EFS is a fully managed, scalable, and highly available network file system that can be shared across many EC2 instances in multiple Availability Zones.
- It is more expensive than Amazon EBS (specifically the gp2/gp3 general purpose SSD volumes), costing roughly 3 times more, and billed based on usage.
- Amazon EFS is a scalable, Linux-compatible, POSIX-compliant file system that supports content sharing across EC2 instances using NFSv4.1, secures data with KMS encryption, and charges based on usage without needing capacity planning.
- Amazon EFS offers storage classes like Standard, Infrequent Access (IA), and Archive to save costs by automatically moving unused files to cheaper tiers using lifecycle policies, with options for high durability (Multi-AZ) or lower-cost (One Zone) setups.

### 188. EFS Modes

- ​Amazon Elastic File System (EFS) offers configurable Throughput Modes and Performance Modes to optimize file system performance based on workload requirements.
- Throughput Modes determine the rate at which data can be read from or written to the file system, while Performance Modes define how the file system handles operations and manages latency for different workloads.
- Throughput is about how much data can flow in and out per second (like the width of a water pipe).
- Performance mode is about how the system handles speed and delays when many users or tasks are using it at once (like how smooth the water flows, even if many taps are open).
- So, one is about data volume per second, the other is about handling many users or complex work smoothly.

### 189. EFS Modes Contd

- The Throughput Modes include Bursting, which scales throughput with storage size; Provisioned, allowing manual specification of throughput independent of storage; and Elastic, which automatically adjusts throughput based on demand. For Performance Modes, General Purpose is suitable for latency-sensitive applications, while Max I/O supports highly parallelized workloads but with increased latency.

### 190. EFS Cost reduction and Access Points

- EFS lets many Linux EC2 instances across Availability Zones share files like WordPress content, costs more than EBS, but supports storage tiers to help reduce cost.
- EFS Access Points make it easy to give different users access to only certain folders in a shared file system, using set user IDs and permissions.

### 191. EFS Operations and Cloudwatch Metrics

- Some EFS changes like lifecycle rules or access points can be done directly, but others like enabling encryption or changing performance mode need DataSync, which copies all the files and their metadata from the original file system to a new one.
- Amazon EFS CloudWatch metrics help monitor performance by showing how close the system is to its I/O limit (PercentIOLimit), how many burst credits are available (BurstCreditBalance), and the total storage used (StorageBytes).

### 192. S3

- Amazon S3 is used for many purposes including backup, disaster recovery, archiving, hybrid storage, hosting apps or media, data analytics, software delivery, and static websites.
- Amazon S3 stores files in region-based "buckets" that must have globally unique names and follow strict naming rules like using only lowercase letters, no underscores, and specific length and format.

### 193. S3 Objects

- In Amazon S3, each file (object) is identified by a key, which is just the full path name (including folders and file name), but folders don't really exist—it's just a long name with slashes.
- Amazon S3 objects can be up to 5TB in size and include the main content, optional metadata, up to 10 tags, and a version ID if versioning is enabled.

### 194. Multipart upload

- **Multipart upload** in Amazon S3 is a way to upload large files (over 5GB) by breaking them into smaller parts, uploading each part separately, and then combining them into one object—this makes uploads faster, more reliable, and allows resuming if a part fails.

### 195. S3 Security

- Amazon S3 security uses user-based IAM policies, resource-based policies like bucket policies and ACLs, requires both allow and no deny for access, and supports encryption using keys.
- S3 bucket policies are written in JSON to control access by specifying allowed or denied actions for users or accounts, and are used to manage public access, enforce encryption, or allow cross-account access.

### 196. S3 Security Contd

- S3 block public access settings help prevent data leaks by blocking all public and cross-account access through ACLs or policies, and should stay enabled unless public access is required.
- Amazon S3 can host static websites accessible via a region-based URL, and a 403 error usually means the bucket policy doesn't allow public read access.

### 197. Amazon S3 versioning

- Amazon S3 versioning lets you keep multiple versions of the same file in a bucket, helping protect against accidental deletion or unwanted changes by allowing you to restore previous versions easily. It is enabled at the bucket level, and every time a file with the same key is uploaded, a new version is created.
- Files uploaded before enabling versioning have a "null" version, and turning off versioning later does not remove existing versions. It is considered best practice to enable versioning for safety and easier file recovery.

### 198. S3 Replication

- Amazon S3 supports replication of files between buckets using Cross-Region Replication (CRR) and Same-Region Replication (SRR), which helps with compliance, faster access, and syncing data between environments like production and testing.
- To use replication, versioning must be enabled on both the source and destination buckets, and the right IAM permissions must be set. Buckets can even belong to different AWS accounts, and the file copying happens asynchronously, meaning there may be a slight delay before the data appears in the destination.

### 199. S3 Replication Contd

- Once Amazon S3 replication is enabled, it only applies to new objects, but existing objects can be copied using S3 Batch Replication, which also handles previously failed replications. For delete operations, you can optionally replicate delete markers, but deletions with version IDs are not replicated to protect against accidental or malicious deletions.
- Also, S3 does not support chained replication—objects from a source bucket will not automatically replicate beyond the first destination bucket, even if that destination has its own replication setup.

### 200. S3 storage classes

- Amazon S3 offers different storage classes to match various access and cost needs, including Standard for general use, Standard-IA and One Zone-IA for infrequent access, Glacier options (Instant-for seconds, Flexible-for hours, and Deep Archive- for days) for archival storage with different retrieval times, and Intelligent Tiering for automatic cost optimization.
- You can move objects between these classes either manually or by setting up S3 Lifecycle configurations to automate transitions based on age or usage patterns.

### 201. S3 Durability and Availability

- Amazon S3 provides extremely high durability of 99.999999999% (11 nines) across multiple Availability Zones, meaning if you store 10 million objects, you might lose one object every 10,000 years. This high level of durability applies to all storage classes.
- Availability measures how often the service is accessible, varies by storage class. For example, S3 Standard offers 99.99% availability, which translates to about 53 minutes of potential downtime per year.

### 202. S3 Lifecycle Rules

- Amazon S3 Lifecycle Rules help manage storage by automatically moving objects to cheaper storage classes after a set time (transition actions) or deleting them when no longer needed (expiration actions); these rules can be based on object names (prefix) or tags like department.
- Standard-IA stores infrequently accessed data across multiple Availability Zones for high availability, while One Zone-IA stores it in a single zone at lower cost but with less fault tolerance, making it cheaper but less resilient to zone failures

### 203. S3 Storage Class Analysis for Cost Optimization

- Amazon S3 Storage Class Analysis helps identify when to move objects from Standard to Standard-IA by providing daily CSV reports, making it a useful starting point for creating or refining lifecycle rules—though it doesn't support One-Zone IA or Glacier classes.

### 204. S3 Event Notifications for Automated Workflows

- Amazon S3 Event Notifications let you trigger actions like Lambda functions, SNS, or SQS when specific events (e.g., object creation or deletion) occur, with support for file filtering and near real-time delivery.
- To allow S3 to send event notifications to SNS, SQS, or Lambda, each destination must have a resource-based policy that explicitly grants permission to the S3 service using the bucket’s ARN.

### 205. S3 Performance

- S3 performance can scale further by using multiple prefixes within a bucket.
- Imagine you have a bucket named my-app-data and you're storing millions of image files. Instead of putting all files in one folder like my-app-data/images/file1.jpg, file2.jpg, etc., you can organize them by prefix to increase performance:

  - my-app-data/images/a/file1.jpg → prefix: images/a/

  - my-app-data/images/b/file2.jpg → prefix: images/b/

  - my-app-data/images/c/file3.jpg → prefix: images/c/

  - my-app-data/images/d/file4.jpg → prefix: images/d/

- Each of these prefixes (images/a/, images/b/, etc.) can handle 5,500 GET requests/sec, so across 4 prefixes you get 22,000 GET requests/sec. This way, S3 performance scales by distributing traffic across multiple prefixes.

### 206. S3 Performance Options

- To boost performance, S3 supports Multi-Part Upload, which splits large files (recommended for >100MB, required for >5GB) into parts that are uploaded in parallel to speed up transfers.
- It also offers S3 Transfer Acceleration, which speeds up long-distance uploads by sending data to the nearest AWS edge location, then forwarding it over the AWS network to the target S3 bucket. Both features can be used together to optimize large file uploads globally.

### 207. S3 Byte-Range Fetches for Faster Access

- S3 Byte-Range Fetches allow you to speed up downloads and improve fault tolerance by requesting specific byte ranges in parallel or retrieving only part of a file, such as its header.

### 208. S3 Inventory

- Amazon S3 Inventory is a tool that creates daily or weekly reports listing the objects in your bucket, including details like size, storage class, encryption status, and more. It helps you keep track of what's stored in your bucket and is especially useful for audits, compliance, and bulk operations like filtering objects for S3 Batch Operations.

### 208. S3 Batch Operations

- S3 Batch Operations let you perform actions like copying, tagging, encrypting, and restoring large sets of objects using job lists filtered with Athena from S3 Inventory, all while managing retries, progress, and notifications.
- S3 Inventory generates a full report of your S3 objects and saves it as a file (CSV, ORC, or Parquet) in a bucket. Athena is then used by you to query and filter that report, for example, to find only certain files (like those stored in Glacier) before passing the results to S3 Batch Operations. So, S3 Inventory creates the data, and Athena helps you search through it.

### 209. Examples of Batch Operations
- For example you can use S3 Batch Operations to add the same tag to 10,000 images in a bucket all at once instead of tagging each one manually.
- Also, you can use S3 Batch Operations to restore hundreds of archived files from S3 Glacier back to standard storage in one go.

### 210. Amazon S3 Glacier for Long-Term Archival
- Amazon S3 Glacier is a low-cost, highly durable storage service for long-term backup and archiving, where data is stored in encrypted "archives" inside "vaults" and is ideal for data retained over decades with lower monthly costs and slower retrieval times.

### 210. Vault Policies and Lock in S3 Glacier
- Every Glacier vault has one rule for who can access it(access policy) and one locked rule(lock policy) that can never change, used to protect data—like stopping files from being deleted for a year or making sure files can only be written once and read many times.

### 211. Glacier Restore Notifications
- Amazon Glacier can send a message via SNS to notify users when a restore job is completed, and S3 can also send event notifications when restoration starts(using s3:ObjectRestore:Post) or finishes using s3:ObjectRestore:Completed.

### 212. Amazon Athena Overview
- Amazon Athena is a serverless service that lets you run SQL queries directly on S3-stored data in various formats like CSV and JSON, costing $5 per TB scanned, and is often used with tools like QuickSight for analytics and reporting.
- To improve Amazon Athena performance, use columnar formats(Columnar formats let Athena read only the columns you query instead of scanning entire rows), compress data, partition datasets in S3, and store larger files over 128MB to reduce scan costs and speed up queries.
- Big files (over 128MB) reduce the number of file scans Athena has to do, which means fewer reads, less overhead, and faster queries—this helps you save money and get results quicker.

### 213. Athena Federated Query
- Athena Federated Query lets you use SQL to get data from different places (like S3, DynamoDB, RDS, or even on-premise databases) by using special connectors that run on Lambda, and it saves the results in S3.

### 214. S3 Object Encryption Methods
- You can encrypt S3 objects using server-side options like SSE-S3 (default), SSE-KMS, SSE-C, or use client-side encryption, depending on who manages the encryption keys and where encryption happens.
- SSE-C encrypts data on the server using customer-provided keys, while client-side encryption encrypts data before sending it to S3, meaning AWS never sees the unencrypted data or the keys.

### 215. Example Difference: SSE-C vs Client-Side Encryption
- With SSE-C, you upload a file to S3 and give your own key for S3 to encrypt it on the server; with client-side encryption, you encrypt the file yourself (e.g., using AWS SDK or a tool) before uploading, so S3 stores only the already-encrypted version.

### 216. SSE-S3 Default Server-Side Encryption
- SSE-S3 encrypts S3 objects on the server side using AES-256 with keys fully managed by AWS, and it's enabled by default for new buckets and objects, and requires the header "x-amz-server-side-encryption": "AES365"

### 217. SSE-KMS for S3 Encryption
- SSE-KMS encrypts S3 objects server-side using AWS Key Management Service keys, giving users more control and audit ability through CloudTrail, and requires the header "x-amz-server-side-encryption": "aws:kms".
- Using SSE-KMS affects your KMS API quota because each upload and download makes a `GenerateDataKey` or `Decrypt` call, which may hit the per-second request limit unless increased through the Service Quotas Console.

### 218. Server-Side Encryption with Customer-Provided Keys
- SSE-C lets you encrypt S3 objects using your own key (not stored by AWS), requiring HTTPS and including the key in each request header.

### 219. Default Encryption vs. Bucket Policies in S3
- SSE-S3 encryption is applied by default to new S3 objects, but you can use bucket policies to enforce stronger encryption types like SSE-KMS or SSE-C by denying any uploads that don’t include the required encryption headers, and note that bucket policies are evaluated before default encryption.

### 220. What is CORS?
- CORS (Cross-Origin Resource Sharing) is a browser mechanism that controls whether a web application from one origin (protocol + domain + port) can request resources from a different origin, and only allows it if the target origin explicitly permits it via special CORS headers like Access-Control-Allow-Origin.

### 221. S3 MFA Delete
- S3 MFA Delete adds extra security by requiring a one-time code (MFA) to permanently delete object versions or suspend versioning, and it can only be enabled by the root account after versioning is turned on.

### 222. S3 Access Logs
- S3 Access Logs record all requests made to an S3 bucket (whether allowed or denied) into another S3 bucket in the same region, mainly for auditing and analysis.

### 223. S3 Pre-Signed URLs
- Pre-signed URLs let users temporarily access private S3 objects with permissions of the URL creator, and can be set to expire within 1 minute to 168 hours using the S3 Console, AWS CLI, or SDK.

### 224. S3 Glacier Vault Lock
- S3 Glacier Vault Lock uses a Write Once Read Many (WORM) model to prevent future changes or deletions to archived data, helping meet compliance and long-term retention requirements.

### 225. S3 Object Lock for Data Protection and Compliance
- Amazon S3 Object Lock lets you prevent an object version from being deleted for a specific time, using a Write Once Read Many (WORM) model. 
- It requires versioning to be enabled and offers two retention modes: **Compliance**, which cannot be changed or bypassed by any user (even the root), and **Governance**, which allows only specific users with permissions to change or delete the object. 
- You can set a **Retention Period** to protect the object for a fixed time (extendable), and also apply a **Legal Hold** to lock the object indefinitely, which can be added or removed using IAM permissions.

### 226. S3 Access Points
- S3 Access Points let you create custom access policies for different users or teams by giving each group its own access point with specific permissions to parts of a shared S3 bucket, making security management easier.

### 227. S3 Access Points – VPC Origin  
- You can make an S3 access point only reachable from inside a VPC by using a VPC endpoint, and the endpoint policy must allow access to both the access point and the S3 bucket.
- A VPC Endpoint Gateway lets private EC2 instances securely access S3 without using the internet

### 228. S3 Multi-Region Access Points
- S3 Multi-Region Access Points let you create one global endpoint that routes traffic to the nearest S3 bucket across regions, keeps data in sync with bi-directional replication, and supports quick failover between regions.

### 229. AWS Snowball
- AWS Snowball is a secure, portable device used to move large amounts of data (up to petabytes) into or out of AWS, with options optimized for storage or compute at the edge.
- AWS Snowball is ideal for migrating large data (over a week’s worth by network) when bandwidth is limited or unstable, avoiding long transfer times, high costs, and connectivity issues.
- Edge computing lets you process data at remote locations (like trucks or ships) using Snowball Edge devices when internet or computing power is limited.

### 230. Amazon FSx 
- Amazon FSx is a fully managed service that lets you run popular third-party high-performance file systems like Lustre, Windows File Server, NetApp ONTAP, and OpenZFS on AWS.

### 231. Amazon FSx for Windows
- Amazon FSx for Windows is a fully managed file system that supports SMB and Windows NTFS, integrates with Active Directory, scales for high performance, supports both SSD and HDD storage, works with Linux EC2, and backs up data daily to S3.
- FSx for Windows Single-AZ replicates data within one availability zone, while Multi-AZ provides automatic failover by synchronously copying data to a standby server in a different zone.

### 232. Amazon FSx for Lustre
- Amazon FSx for Lustre is a high-speed, scalable file system built for large-scale computing like machine learning and video processing, with seamless S3 integration and support for both SSD and HDD storage.
- FSx Lustre offers Scratch File Systems for fast, temporary processing without data replication, and Persistent File Systems for reliable, long-term storage with automatic replication and file recovery.

### 233. Amazon FSx for NetApp ONTAP
- FSx for NetApp ONTAP lets you use NetApp file systems on AWS to store and access files from different systems like Windows, Linux, and Mac, and it grows or shrinks storage as needed, supports fast backups, and helps move data from old systems to the cloud with automatic scaling, snapshots, and instant cloning.

### 234. Amazon FSx for OpenZFS
- FSx for OpenZFS lets you run fast, flexible ZFS storage in the cloud that works with Linux, Windows, Mac, and other AWS services, and includes features like snapshots, fast speeds, and easy cloning.

### 235. AWS Storage Gateway 
- AWS Storage Gateway helps you connect your on-premises storage to AWS cloud storage so you can back up data, recover files, and access them faster, using tools like file, volume, or tape gateways.

### 236. S3 File Gateway
- S3 File Gateway helps you use S3 storage like a regular hard drive(using NFS or SMB) from your office, keeps recent files close for faster access, and moves old files to cheaper storage over time(i.e. Glacier using lifecycle rules.).

### 237. Volume Gateway
- Volume Gateway lets you store and back up your on-premises data to AWS using block storage, with fast access to recent files (cached volumes) or full local storage with cloud backups (stored volumes).
- S3 File Gateway is for working with files, while Volume Gateway is for working with block storage (like hard drives). It’s better for applications that need block-level access like databases. Think of S3 File Gateway like a shared folder, and Volume Gateway like a virtual hard drive.
- Operating system disks, Database storage, Virtual machine disks are examples of volumes made up of blocks.

### 238. Simple difference between files and block storage:
- Files are like full documents you save and open — like photos, PDFs, or Word files. You deal with the whole file at once, and it's stored in folders just like on your computer. S3 File Gateway works with this kind of storage.
- Block storage is like breaking a file into tiny pieces (blocks) and storing those pieces across a hard drive. The system puts the blocks together when needed. It’s more like how a computer’s hard drive works. Volume Gateway works with this kind of storage.
- Files = whole items, easy to see and use (good for backups, sharing, etc.)
- Blocks = tiny pieces used behind the scenes (good for databases or systems that need fast, low-level access).

### 239. Tape Gateway
- Tape Gateway lets companies keep using their old tape backup systems by turning physical tapes into virtual ones stored in Amazon S3 and Glacier, without changing their backup software or process.

### 240. Storage Gateway Hardware Appliance
- Storage Gateway Hardware Appliance is a physical device you can buy and use to run AWS File, Volume, or Tape Gateway locally without needing your own Storage Gateway virtualization setup.

### 241. AWS Storage Gateway Overview
- It connects on-premises systems (file shares, app servers, backups) to AWS Cloud using File, Volume, or Tape Gateways, securely syncing and storing data in services like S3, EBS, Glacier, and FSx.

### 242. File Gateway Posix support
- File Gateway works well with Linux(supports POSIX metadata for Linux file systems) and can be restarted easily, but Volume and Tape Gateways need a few extra steps to restart using the AWS Console or tools.
- POSIX means the File Gateway understands Linux-style file details like who owns the file, file permissions, and timestamps (when it was created or changed), and it saves this info with the file in S3.

### 243. POSIX Restart
- When a File Gateway is restarted, POSIX support ensures that file ownership, permissions, and timestamps are remembered correctly, because that info is stored with the files in S3. So after a restart, everything works just like before — users still have the right access, and nothing breaks.
- In contrast, Volume and Tape Gateways don’t use POSIX, so they don’t track these file-level details. Instead, they deal with blocks of data, like a raw hard drive or a virtual tape. Restarting them is a bit more complex and may require manual steps through the AWS Console or API to safely reconnect and mount the volumes or tapes.

### 244. POSIX Example
- Imagine you have a shared folder used by three people on a Linux server — Alice, Bob, and Carol. Alice uploads a file called report.txt using File Gateway. Because File Gateway is POSIX-compliant, it saves the owner (Alice), permissions (e.g., only Alice can edit), and timestamp with the file in S3.
Now, when Bob and Carol open the shared folder, the Linux system can check:
Who owns the file? → Alice,
Who is allowed to read or write it? → Maybe only Alice,
When was it last modified? → It shows the correct time.
- Without POSIX support, these details could be missing or incorrect, and the system might not enforce the right rules, which could lead to mistakes or security issues.

### 245. Tape Gateway and Volume Gateway POSIX
- Tape Gateway and Volume Gateway do not have this POSIX advantage.
- They focus on block storage (like hard drives) and backup systems, not file-level details. That means they don’t store or understand Linux-style file info like:
Who owns the file,
File permissions (read/write access),
Timestamps for when the file was created or modified.
- This is fine for things like backups or database storage, where the system cares more about the data blocks than the file permissions. But if you're sharing files across users or need Linux-style control, File Gateway with POSIX support is the better choice.

### 246. Storage Gateway Activation
- To start using Storage Gateway, you get a special code (activation key) either by typing a command(Gateway VM CLI) or visiting a link(web request to port 80), but make sure port 80 is open and the computer time is correct by syncing with a time server.

### 247. Amazon CloudFront
- CloudFront is a Content Delivery Network (CDN) that makes websites load faster by storing copies of content at global edge locations, improving user experience, reducing latency, and protecting against DDoS attacks.
- By using many edge locations and AWS Shield, CloudFront spreads traffic across the network and blocks large or suspicious traffic before it reaches the main server, stopping DDoS attacks early.

### 248. CloudFront Origins
- CloudFront can deliver content from an S3 bucket (with enhanced security using Origin Access Control) or any custom HTTP origin like an EC2 instance, load balancer, or S3 static website.

### 249. CloudFront vs S3 Cross Region Replication
- Cloudfront is best for delivering static content globally using a cached edge network with short refresh times. S3 Cross Region Replication is better for keeping dynamic content up-to-date in **specific**(i.e a few regions) regions, though it must be **set up per region** and is read-only and **no caching**.

### 250. CloudFront Geo Restriction
- CloudFront Geo Restriction lets you control who can access your content based on their country, using allowlists or blocklists checked through a Geo-IP database. It is useful for following copyright rules.

### 251. CloudFront Access Logs
- CloudFront Access Logs record every user request made to CloudFront and store the log files in a designated S3 bucket for monitoring and analysis.

### 252. CloudFront caching
- CloudFront caching stores content at edge locations based on headers, cookies, and query strings, reducing origin requests by using a TTL and allowing cache control or invalidation through headers or APIs.
- CloudFront handles headers by either forwarding all (no caching), forwarding a selected few (partial caching/whitelisted headers), or forwarding none (best caching), depending on how you want caching behavior to work.

### 253. User Agent Header
- The User-Agent is a piece of information your browser or app sends to a website to tell it what kind of device or software you're using — like "I’m Chrome on a Windows laptop" or "I’m Safari on an iPhone." This helps websites adjust how they show content based on your device.
- If you forward all headers, like User-Agent, CloudFront will send every request to your origin because it thinks each device is different — no caching.
- If you forward only the User-Agent header(dont forward other headers), CloudFront will cache one version for mobile and one for desktop — some caching.
- If you forward no headers, CloudFront will always serve the same image to everyone — fastest caching.

### 254. CloudFront origin custom headers vs Cache behavior 
- CloudFront origin custom headers are special headers that CloudFront always adds to requests when sending them to your origin server, and they always have the same fixed value, no matter what the user sends.
- They are like a fixed message CloudFront sends to your server every time, to identify or control behavior, while cache behavior settings decide which request headers are forwarded and affect caching.
-  CloudFront origin custom headers let your origin know the request came from CloudFront, they control access or respond differently based on a fixed header. They are also used to enable debugging or add extra metadata for your backend.

### 255. Origin headers
- Cache-Control Header: This tells CloudFront (or any browser) how long it should keep a copy of the content. It uses a time value like max-age=3600, which means "keep this for 3600 seconds (1 hour) before checking for a new version."
- Expires Header: This gives a specific date and time when the content should be considered "old." For example, `Expires: Wed, 15 Apr 2025 12:00:00 GMT` means "after this time, check if there's a newer version."
- Cache-Control is newer and more flexible. CloudFront prefers it over Expires if both are present.Both Cache-Control and Expires are origin response headers — meaning they are set by the origin server (like S3, EC2, or an external web server) and sent back to CloudFront when it fetches the content.
- They help control how CloudFront and browsers cache and reuse that content. If your origin doesn’t set them, CloudFront uses its own default caching settings that you can configure.

### 256. Cookies effect on Cache
- CloudFront handles cookies by either ignoring them (default, best caching), forwarding only selected ones (whitelist, moderate caching), or forwarding all cookies (worst caching). When ignored, caching is not based on cookie values, so more users get the same cached response. 
- If you whitelist certain cookies, caching depends on those specific values, which reduces cache efficiency but allows some personalization. Forwarding all cookies makes every request unique, preventing effective caching and increasing load on the origin server.

### 257. Query strings effect on Cache
- CloudFront handles query strings by either ignoring them (best caching), forwarding selected ones (custom caching), or forwarding all (least efficient caching), which affects how it stores and serves cached content.

### 258. Improve Cache performance
- To improve performance, CloudFront separates static and dynamic content so static files are heavily cached while dynamic content is processed using custom headers, cookies, or logic.
- To increase the cache ratio in CloudFront, you should monitor the `CacheHitRate` in CloudWatch and make caching more effective by setting how long content should be cached using the `Cache-Control max-age` header. 
- You should also reduce the number of variations in cached content by forwarding none or only the necessary headers, cookies, and query string parameters. 

### 259. Using sticky sessions with CloudFront and an Application Load Balancer
- To use sticky sessions with CloudFront and an Application Load Balancer (ALB), you must whitelist and forward the special cookie (`AWSALB`) that keeps users connected to the same backend server, enabling session affinity. 
- This ensures users stay on the same target in a target group for a consistent experience. Also, you need to set a Time-to-Live (TTL) value that is shorter than the expiration time of the authentication cookie to avoid session mismatches during caching.

### 260. Amazon RDS
- Amazon RDS is a managed service by AWS that lets you create and run SQL-based relational databases like MySQL, Postgres, Oracle, and Aurora in the cloud.
- RDS is better than deploying a database on EC2 because it offers automatic backups, scaling, monitoring, and disaster recovery, but you can't access it directly with SSH.
- RDS storage auto scaling automatically increases your database storage when it's running low, based on conditions like free space and time, making it ideal for apps with changing workloads.

### 261. RDS Read Replicas
- RDS Read Replicas let you scale read traffic by creating up to 15 asynchronously updated copies of your database across different zones or regions, which can be promoted to standalone databases if needed.
- RDS Read Replicas let you offload reporting or analytics tasks from your main production database by handling read-only(not INSERT, UPDATE or DELETE) queries separately, ensuring the main application remains unaffected.

### 262. RDS Read Replicas vs RDS Multi-AZ
- RDS Read Replicas incur no network cost within the same AWS region, but transferring data across regions results in extra charges.
- RDS Multi-AZ provides high availability and automatic failover using synchronous replication between primary and standby instances, but it's not meant for scaling. The RDS Read Replicas can be setup as Multi AZ for Disaster Recovery.
- You can switch from Single-AZ to Multi-AZ in RDS with zero downtime by clicking on modify database and enabling multi-az(this will take a snapshot, restoring it in a new Availability Zone, and enabling synchronous replication between the two databases.)

### 263. RDS Multi-AZ failover
-  RDS Multi-AZ failover happens automatically if the primary DB fails, becomes unreachable, is being patched, modified, or unresponsive, or during AZ outages, and can also be triggered manually using “Reboot with failover.

### 264. AWS Lambda and VPC
- By default, AWS Lambda runs outside your VPC, so it cannot access private VPC resources like RDS, ElastiCache, or internal ELBs.
- To allow a Lambda function to access resources in a VPC, you must specify the VPC ID, subnets, and security groups. Lambda uses this information to create an Elastic Network Interface (ENI) in the specified subnet, which lets it connect to private resources in that VPC.
- Even though the ENI is created in a specific subnet, that subnet is part of the VPC, and once inside the VPC, the Lambda function can access any private resource in that VPC (as long as the security group rules allow it). This setup requires the AWSLambdaVPCAccessExecutionRole.

### 265. RDS Proxy
- RDS Proxy helps Lambda functions manage database connections to RDS efficiently by pooling these connections, preventing too many open connections, and allowing secure access using IAM or DB authentication.

### 266. DB Parameter Groups
- DB Parameter Groups let you configure database settings, where dynamic changes apply immediately, static ones need a reboot, and important parameters like rds.force_ssl=1 or require_secure_transport=1 help enforce secure (SSL) connections.

### 267. RDS backups vs Snapshot
- RDS backups happen automatically during maintenance windows, are continuous(doesnt stop), and allow point-in-time recovery within a set retention period (0–35 days), while snapshots are manual or user-triggered, can briefly impact performance, are incremental after the first full one, can be shared or copied, and do not expire unless deleted manually.
- RDS backups are called continuous because AWS takes a full backup once a day and then saves every change you make during the day. If something goes wrong, it can rewind your database to any exact moment—like going back in time((0–35 days))—to fix the problem.

### 268. Sharing snapshots
- You can share manual RDS snapshots (not automated ones) with other AWS accounts, but only if they’re unencrypted or encrypted using a customer-managed key that is also shared.

### 269. RDS events and Metrics
- RDS tracks events like DB status changes and lets you subscribe to these events using SNS or EventBridge to get notifications when they happen.
- CloudWatch monitors key RDS metrics like connections, IOPS, latency, and storage, while Enhanced Monitoring gives deeper CPU and system-level insights using an agent on the DB instance.

### 270. RDS Performance Insights
- RDS Performance Insights helps you see what's slowing down your database by showing which users, servers, SQL statements, or resources are causing the most load, using a simple visual dashboard.

### 271. Amazon Aurora
- Amazon Aurora is a high-performance, cloud-optimized database from AWS that supports MySQL and Postgres, scales storage automatically, offers fast replication and failover, and costs more than RDS but is more efficient and highly available.
- Amazon Aurora provides high availability and read scaling by storing six copies of data across three Availability Zones, allowing fast failover in under 30 seconds, supporting up to 15 read replicas, and enabling cross-region replication.

### 272. An Aurora DB Cluster
- An Aurora DB Cluster uses a writer endpoint for writes to the master and a reader endpoint for load-balanced reads from replicas, with shared storage that auto-expands from 10 GB to 128 TB.
- Aurora lets you set a failover priority (0–15) for each read replica, promoting the one with the highest priority (lowest number), and you can also migrate an RDS MySQL snapshot into an Aurora MySQL cluster.

### 273. Aurora CloudWatch metrics
- Aurora CloudWatch metrics like `AuroraReplicaLag`, `DatabaseConnections`, and `InsertLatency` help monitor replication delays, connection counts, and insert performance, which is important because high replica lag can lead to inconsistent user experiences due to delayed data synchronization.

### 274. RDS and Aurora Security
- RDS and Aurora keep your data safe by encrypting it when stored or sent(at-rest and in-flight ), let you connect using IAM instead of passwords, block unwanted access with security groups, don’t allow SSH (except in special cases), and can save logs for later checks using CloudWatch.

### 275. Amazon ElastiCache
- Amazon ElastiCache is a managed in-memory caching service (Redis or Memcached) that boosts performance by reducing database load, but it requires major changes to your application code.
- Apps checks ElastiCache for data first, and if it's not found, it gets the data from RDS, saves it in the cache, and uses it—helping reduce load on the database.

### 276. ElastiCache Redis vs ElastiCache Memcached
- ElastiCache stores user session data so that when a user logs in on one app instance, other instances can quickly retrieve the session and keep the user logged in.
- Redis supports high availability, replication, and data durability, while Memcached is simpler with fast, non-persistent, multi-threaded performance but lacks built-in replication or failover.

### 278. Redis Cluster Mode Enabled and Redis Cluster Mode Disabled
- The main difference is that Redis cluster mode enabled splits data across multiple shards for better write and read scalability, while cluster mode disabled keeps all data in one shard, limiting scalability.
- A shard is a logical concept. It can be stored on one server or spread across several.Imagine a shard is a drawer of files. The EC2 instance is the cabinet that holds the drawer. You can have one drawer (shard) per cabinet (EC2), or even more, but the drawer (shard) is about the data grouping, not the machine itself. 
- A shard is not the EC2 instance, but it's often managed by a Redis process running on an EC2 instance.

| Feature                        | Cluster Mode Disabled                       | Cluster Mode Enabled                         |
|-------------------------------|--------------------------------------------------|--------------------------------------------------|
| Data distribution         | All nodes in one shard (full data on each node) | Data is split across multiple shards             |
| Write scalability         | Limited (only one primary node writes)          | High (multiple shards = multiple write targets)  |
| Max nodes per cluster     | 6 (1 primary + up to 5 replicas)                | 500 (up to 500 total nodes across shards)        |
| Replication               | All replicas copy from one primary              | Each shard has its own primary and replicas      |
| High availability         | Yes (Multi-AZ with failover)                    | Yes (Multi-AZ with failover per shard)           |
| Best for                  | Simple cache with limited scaling               | Large-scale, high-performance cache needs        |

### 279. Redis Evictions and other metrics
- Evictions in Redis happen when the memory is full, and Redis needs to make space for new data by removing existing data — usually the least recently used (LRU) items or based on another policy you set. It only removes non-expired items, and it means your cache is too full, so you may need to scale up (more memory) or out (more nodes).
- To monitor Redis health, track evictions (indicating memory pressure), CPUUtilization (host performance), and SwapUsage (should stay under 50 MB), and scale resources or optimize settings if these metrics are too high.

### 280. Redis SwapUsage
- Disk is the storage on your computer (like a hard drive or SSD) where data is saved for a long time, even when the computer is turned off. Memory (RAM) is much faster but temporary — it only stores data while the computer is on. In Redis, we want to use memory because it’s fast. If it runs out, Redis might start using disk, which makes it much slower.
- SwapUsage means how much Redis is using the disk instead of memory. This happens when the memory is too full. Using the disk is slow, so Redis can become slow too. To fix this, make sure you give Redis enough memory or use a bigger server.

### 281. CloudWatch Metrics
- CloudWatch helps you monitor how well AWS services are working by using metrics such as CPU or network usage. These metrics are organized into namespaces, and each one can have extra details called dimensions, like the ID of the instance being measured. 
- Metrics also include timestamps to show when the data was recorded, and you can view all this information clearly using CloudWatch dashboards.
- EC2 detailed monitoring lets you get instance metrics every 1 minute instead of every 5 minutes (for a cost), helps with faster scaling, and memory metrics must be added manually.
- CloudWatch Custom Metrics let you track your own data (like RAM or disk use) by sending it to CloudWatch, where you can label it, choose how often it updates, and make sure it's timed properly.

### 282. CloudWatch Custom Metrics
- CloudWatch Custom Metrics allow you to send your own metrics (like RAM or disk usage) using the PutMetricData API, with options for custom dimensions and resolutions, and support for metric data up to 2 weeks old or 2 hours ahead.

### 283. CloudWatch Dashboards
- CloudWatch Dashboards let you create and share global, customizable, auto-refreshing dashboards with graphs from multiple AWS accounts and regions, with free access for 3 dashboards and a cost of \$3 per dashboard per month afterward.

### 284. CloudWatch Logs
- CloudWatch Logs allow you to organize logs using log groups (representing applications) and log streams (individual sources like files or containers). 
- You can set custom log expiration policies ranging from 1 day to 10 years or never expire. 
- Logs can be exported or streamed to services like Amazon S3, Kinesis Data Streams, Kinesis Firehose, AWS Lambda, or OpenSearch. - By default, logs are encrypted, and you can enhance security further by configuring KMS-based encryption with your own keys.

### 285. CloudWatch Logs Sources
- CloudWatch Logs can collect data from various sources such as SDKs, the CloudWatch Logs Agent, and the Unified Agent, as well as AWS services like Elastic Beanstalk (application logs), ECS (container logs), AWS Lambda (function logs), and VPC Flow Logs (network traffic). Logs can also come from API Gateway, filtered events in CloudTrail, and DNS queries via Route 53, making it a comprehensive logging solution across the AWS ecosystem.

### 286. CloudWatch Logs Insights
- CloudWatch Logs Insights is a query engine that lets you search and analyze log data across multiple log groups using a special query language, enabling filtering, statistics, and dashboard integration, but it is not designed for real-time monitoring.

### 287. CloudWatch Alarms 
- CloudWatch Alarms are used to monitor AWS metrics and send notifications when certain conditions are met, using different options such as sampling, percentages, or threshold values.
- Sampling means checking a subset of data points over a time period, percentages involve triggering the alarm based on a certain percentage of data points breaching a condition, and threshold values are specific limits (like CPU > 80%) that, when crossed, cause the alarm to change state.
- Alarms can be in one of three states: OK, ALARM, or INSUFFICIENT_DATA.
- Alarm Period defines the time in seconds over which the metric is evaluated, and high-resolution custom metrics can be set to intervals like 10 seconds, 30 seconds, or any multiple of 60 seconds for more precise

### 288. CloudWatch Alarms Targets
- CloudWatch Alarms can take action by stopping, terminating, rebooting, or recovering EC2 instances, triggering Auto Scaling, or sending notifications through Amazon SNS.
- Composite Alarms in CloudWatch monitor the state of multiple individual alarms using AND/OR logic to reduce unnecessary alerts and trigger actions like SNS notifications only when specific combined conditions are met.
- EC2 Instance Recovery uses CloudWatch alarms to watch your instance to check if it’s working properly, and if the hardware fails, it can automatically fix the problem and restart the instance without changing its IP address or settings.

### 289. CloudWatch Alarms Testing and Logs
- CloudWatch Alarms can be created from specific patterns in log data using metric filters, which allow you to monitor log events and trigger alarms when certain conditions are met. 
- To test if your alarm and notification setup works correctly, you can manually set the alarm state to "ALARM" using the AWS CLI command, which helps simulate an alert without needing a real issue to occur.

## 290. CloudWatch Synthetics Canary
- CloudWatch Synthetics Canary uses configurable scripts to simulate user actions and monitor the availability and performance of APIs or websites, helping detect issues early and trigger alarms or automated responses before these issues are encountered by users.
- CloudWatch Synthetics Canary Blueprints provide ready-made scripts to monitor website and API health, check broken links, compare screenshots, record user actions, and test web workflows like login forms.

### 291. Events
- Events in AWS are messages that describe something that happened in your AWS environment or application. For example, "an EC2 instance started," "a file was uploaded to S3," or "a user signed in." These events are automatically created by AWS services, custom apps, or partner apps.
- Amazon EventBridge resource-based policies let you control which AWS accounts or regions can send events to your event bus, making it easy to collect and manage events across accounts or within an organization.


### 292. Amazon EventBridge
- Amazon EventBridge (formerly CloudWatch Events) lets you run scheduled tasks or respond to specific service events by triggering Lambda functions or sending messages through SQS or SNS.
- Amazon EventBridge supports default, partner, and custom event buses to route events from AWS services, third-party apps, or custom applications, with features like cross-account access, event archiving, and replay.
- You can turn on event archiving for any event bus (default, custom, or partner). This lets you store all or filtered events for later use. You can choose to keep them forever or for a limited time. Saved events can be replayed later to simulate past activity—useful for debugging or testing.

### 293. EventBridge Schema Registry
- Amazon EventBridge Schema Registry helps you understand the structure of your event data, so you can automatically create code that knows what kind of data to expect, and it keeps track of changes with versions.
- For example, when a user signs up, EventBridge captures the event data (like username and email) and creates a schema that defines its structure. This schema can then be used to automatically generate code, making it easier for developers to process the event correctly and track any future changes.

### 294. Service Quotas CloudWatch Alarms
- Service Quotas CloudWatch Alarms notify you when you're close to reaching a service limit (like Lambda concurrency), helping you take action such as requesting a quota increase or reducing usage before hitting the threshold.
- AWS Trusted Advisor can also check service limits and send the results to CloudWatch, where you can set up CloudWatch Alarms to get alerts when usage gets close to those limits.

### 295. AWS CloudTrail
- AWS CloudTrail automatically records all API activity in your account for governance and auditing, and lets you view or store logs in CloudWatch or S3 across all regions or just one.
- CloudTrail logs **management events** by default (like IAM or EC2 actions), while **data events** (like S3 or Lambda activity) are optional due to high volume, and both can be separated into read and write types.

### 296. CloudTrail Insights
- CloudTrail Insights detects unusual account activity by analyzing write events against normal behavior, then logs anomalies to the CloudTrail console, S3, or sends them to EventBridge for automated actions.
- CloudTrail stores events for 90 days by default, but for long-term retention and analysis, you can log them to an S3 bucket and query them using Athena.

### 297. Amazon EventBridge can intercept API calls
- Amazon EventBridge can intercept API calls logged by CloudTrail—like a user deleting a DynamoDB table—and trigger alerts via SNS for immediate notification.
- Amazon EventBridge and CloudTrail work together to detect sensitive actions like assuming IAM roles or modifying EC2 security groups, then send real-time alerts via SNS.

### 298. Cloudtrail digest files
- CloudTrail creates special files called digest files that store a secure fingerprint of each log file to ensure logs haven't been changed after delivery, and recommends securing the S3 bucket and CloudTrail with IAM and protective settings.
- These settings make sure your logs stay safe and can’t be tampered with. e.g. Bucket policies to control who can access the logs, Versioning to keep old copies of files, MFA Delete to require extra approval before deleting logs, Encryption to protect log data, Object lock to prevent logs from being changed or deleted, IAM roles and policies to control who can manage CloudTrail itself.

### 299. CloudTrail EventBridge Response
- CloudTrail can trigger EventBridge to respond to API calls using services like Lambda, SNS, or SQS, but it is not real-time—it delivers events within 15 minutes and log files every 5 minutes.

### 300. CloudTrail Organization Trails
- CloudTrail Organization Trails let the management account log all events from every account in the AWS Organization to a central S3 bucket, while member accounts can only view the trail and cannot change or delete it.

### 301. AWS Config
- AWS Config helps track configuration changes and compliance of AWS resources over time, sends alerts for changes, works per region, and can store data in S3 for analysis with Athena.
- AWS Config Rules let you use or create rules to check resource compliance, trigger evaluations on changes or schedules, but they don’t block actions and have a small per-region cost with no free tier.
- AWS Config lets you track a resource’s compliance, configuration changes, and related CloudTrail API activity over time.

### 302. Config and SSM Automation Documents
- AWS Config can automatically fix non-compliant resources using SSM Automation Documents, including retries if the issue remains, by triggering predefined or custom remediation actions like deactivating expired IAM keys.
- AWS Config can trigger notifications through EventBridge or SNS when resources become non-compliant or undergo configuration changes, allowing admins to stay informed and take action.

### 303. AWS Config Aggregators
- AWS Config Aggregators allow a central account to collect and view compliance data from multiple AWS accounts and regions, simplifying monitoring without needing separate authorization when using AWS Organizations.

### 304. CloudWatch, CloudTrail and Config
- CloudWatch monitors performance and sends alerts, CloudTrail records all API activity in your account, and AWS Config tracks configuration changes and checks compliance over time.
- For an Elastic Load Balancer, CloudWatch monitors traffic and errors, AWS Config tracks configuration and compliance like SSL use, and CloudTrail records who made changes through API calls.

### 305. AWS Health Dashboard
- The AWS Health Dashboard (formerly AWS Service Health Dashboard) displays the health of all services across all regions, includes daily historical status, and provides an RSS feed for updates.
- The AWS Account Health Dashboard (formerly Personal Health Dashboard) provides personalized alerts, guidance, and real-time updates on AWS service issues affecting your resources, including across an entire AWS Organization.
- AWS Health Dashboard = News for everyone. It shows general issues that affect anyone using AWS (e.g. “EC2 in us-east-1 is having problems”). It’s public and not specific to your account.
- AWS Account Health Dashboard = Personal notifications. It shows issues that affect your AWS account or your company's resources. For example, if your EC2 instance is scheduled for maintenance, it tells you only, not everyone.

### 306. AWS Health Event Notifications
- AWS Health Event Notifications use EventBridge to detect changes in your account's health events and trigger automated actions like sending emails, logging data, or alerting via Slack through services like Lambda, SNS, SQS, or Kinesis.
- The AWS Health Dashboard can trigger automated responses via EventBridge—such as deleting exposed IAM access keys with Lambda or restarting EC2 instances scheduled for retirement.

### 307. AWS Organizations
- AWS Organizations is a global service that lets you centrally manage multiple AWS accounts with one billing method, share resources and discounts, and automate account creation using APIs.
- AWS Organizations allows better account management with centralized logging, tagging, cross-account roles, and enhanced security using Service Control Policies (SCPs) that restrict access across organizational units but exclude the management account.

### 308. Service Control Policies
- Service Control Policies (SCPs) are organization-wide permission boundaries in AWS that define what services and actions accounts within an AWS Organization can or cannot use, regardless of their individual IAM policies.
- Think of SCPs as guardrails that limit what AWS services and actions accounts in your organization can use. Even if an account's IAM user or role has permission to do something, the SCP can block it. They're managed centrally and don’t affect the management (root) account. If there's a conflict, a "Deny" in the SCP always wins.

### 309. Blocklist and Allowlist strategy
- Blocklist strategy is a type of Service Control Policy (SCP) strategy in AWS Organizations where all actions are allowed by default, but specific services or actions are explicitly denied to limit access.
- The allowlist strategy explicitly allows only specific services or actions, such as EC2 and CloudWatch, and blocks everything else by default.
![Blocklist and Allowlist strategy](./img/block%20and%20allo%20list.png)

### 310. AWS Organizations Sharing Reserved Instance
- AWS Organizations allows all accounts under consolidated billing to share Reserved Instance and Savings Plans discounts unless the payer account disables this sharing for specific accounts.

### 311. PrincipalOrgID condition and Tag Policies
- Using the aws:PrincipalOrgID condition key in resource-based policies allows you to restrict access to resources (like S3 buckets) to only IAM principals within your AWS Organization.
- Tag Policies in AWS Organizations help enforce consistent tagging across resources by defining allowed tag keys and values, supporting cost allocation, access control, compliance monitoring, and automated reporting.

### 312. AWS Control Tower 
- AWS Control Tower makes it easy to set up, govern, and monitor a secure multi-account AWS environment by automating setup, policy enforcement, and compliance using best practices and Service Control Policies through AWS Organizations.

### 313.  AWS Service Catalog
- AWS Service Catalog lets admins create a self-service portal of pre-approved, compliant AWS resources (like VMs, databases, etc.) so users can easily launch only authorized products.
- AWS Service Catalog allows sharing portfolios across accounts or organizations either as a reference (to stay in sync) or as a copy (requiring manual updates), with the ability to launch and extend imported products locally.
- This means with AWS Service Catalog, you can centrally create and manage sets of approved cloud resources (called portfolios) and then share them with other AWS accounts. You can either let them use your version (reference, which stays updated automatically) or give them a copy (which they must manage and update themselves). This helps maintain control while giving flexibility.

### 314. AWS Service Catalog TagOptions Library
- AWS Service Catalog TagOptions Library allows you to define and manage key-value tag pairs for consistent resource tagging, which can be applied to portfolios and products and shared across accounts and organizations.
- This means TagOptions are like preset labels (key-value pairs) you create to ensure everyone tags AWS resources the same way. These tags can be linked to Service Catalog products and portfolios and reused across accounts, helping you keep resource organization, cost tracking, and compliance consistent.

### 315. AWS Billing Alarms
- AWS Billing Alarms use CloudWatch in the us-east-1 region to track actual global AWS costs (not project-specific) and help notify users when spending exceeds a set threshold.
- AWS Cost Explorer helps you visualize, analyze, and manage your AWS costs and usage over time, create custom reports, choose savings plans, and forecast future usage up to 12 months.

### 316. AWS Budgets
- AWS Budgets helps you track and control your cloud costs. You can set a limit for spending or usage, and get up to 5 alerts (emails or texts) if you’re getting close to your limit. You can also filter by things like service or region, and check how well your Reserved Instances or Savings Plans are being used. It’s free for the first 2 budgets, then $0.02 per day for each extra one.
- AWS Budgets is like a more detailed version of billing alarms, letting you track specific services, regions, or usage with multiple alerts, while billing alarms only watch your total account cost.

### 317. Cost allocation tags
- Cost allocation tags help track AWS costs in detail using AWS-generated tags (prefixed with `aws:`) and user-defined tags (prefixed with `user:`) to categorize resource expenses effectively.
- They help you see exactly where your money is going in AWS. For example, if you tag resources by project, team, or environment (like dev/test/prod), you can break down your bill and know which project or team is spending the most. Without tags, your AWS bill is just one big number with no detail.
- You need both AWS and user-defined tags because AWS tags give automatic system info, they are automatically created by AWS for some services. Example: aws:createdBy shows who launched a resource. You can’t change these, but they give useful built-in info., while your tags let you organize and track spending your own way — together they make your AWS bill clear and useful.

### 318. AWS Cost and Usage and AWS Compute Optimizer
- AWS Cost and Usage Reports provide the most detailed data on AWS spending and usage, exportable daily to S3, and can be analyzed using Athena, Redshift, or QuickSight.
- AWS Compute Optimizer uses machine learning and CloudWatch metrics to recommend optimal resource configurations for EC2, Auto Scaling, EBS, and Lambda, helping reduce costs and improve performance by up to 25%.

### 319. DataSync and Backup
- AWS DataSync moves large amounts of data between on-premises, other clouds, and AWS storage services (S3, EFS, FSx) with support for scheduled transfers, metadata preservation, and high-speed throughput using agents.
- AWS Backup is a fully managed service that automates and centrally manages backups across many AWS services, supporting both cross-region and cross-account backups without needing custom scripts.
- AWS Backup lets you set up automatic or manual backups, choose how often to back up, how long to keep them, and when to move them to cheaper storage, all using simple rules called Backup Plans. It also support Point-In-Time Recovery, meaning you can restore your data exactly as it was at a specific moment in the past.

### 320. AWS Backup Vault Lock
- AWS Backup Vault Lock enforces a WORM (Write Once Read Many) policy to prevent deletion or modification of backups, even by root users, adding strong protection against accidental or malicious data loss.

### 321. AWS Shared Responsibility Model
- The AWS Shared Responsibility Model means AWS secures the cloud infrastructure, while customers are responsible for securing what they put in the cloud (like data, OS, and permissions), with some controls being shared.
- For RDS, AWS manages the infrastructure, patching, and system audits, while you handle security group rules, database access, SSL settings, and encryption.
- SSL (Secure Sockets Layer) settings ensure that data sent between your database and client is encrypted, protecting it from being read or tampered with during transmission.
- For Amazon S3, AWS ensures secure and scalable infrastructure, while you are responsible for configuring the bucket, setting access policies, managing IAM roles, and enabling encryption.

### 322. DDoS (Distributed Denial-of-Service) attack
- A DDoS (Distributed Denial-of-Service) attack is when an attacker uses many compromised computers (bots) to overwhelm a server with traffic, making it unavailable to normal users.
- AWS provides DDoS protection through AWS Shield Standard (free, basic protection) and AWS Shield Advanced (paid, 24/7 enhanced protection). 
- AWS WAF filters incoming traffic based on custom rules, and CloudFront with Route 53 helps absorb attacks using global edge locations. 
- Auto Scaling is also recommended to handle traffic spikes during an attack.
- SYN/UDP floods are types of DDoS attacks where attackers overload a server with a massive number of fake SYN or UDP requests, consuming its resources and making it unresponsive to real users.

### 323. AWS Shield
- AWS Shield provides two levels of DDoS protection: Shield Standard (free, for all customers) protects against common attacks like SYN/UDP floods, while Shield Advanced (paid) offers stronger protection, 24/7 support, and cost safeguards for high-risk services like EC2 and CloudFront.

### 324. AWS WAF (Web Application Firewall)
- AWS WAF (Web Application Firewall) protects web applications from common HTTP-based attacks like SQL injection and XSS by filtering traffic using customizable rules on services like ALB, API Gateway, or CloudFront.

### 325. 
- AWS allows customers to perform penetration testing on 8 specific services like EC2, RDS, and Lambda without prior approval, making it easier to assess cloud security.
- AWS allows customers to perform penetration testing on 8 specific services like EC2, RDS, and Lambda without prior approval, making it easier to assess cloud security.

### 326. Amazon Inspector
- **Amazon Inspector** automatically checks EC2 instances, container images in Amazon ECR, and Lambda functions for security issues like network exposure and known vulnerabilities, then sends the results to Security Hub or EventBridge.
- Amazon Inspector continuously scans EC2, ECR, and Lambda for package vulnerabilities and network reachability, assigning a risk score to help prioritize security fixes.

### 327. Logs
- AWS provides various logs like CloudTrail, VPC Flow Logs, and CloudWatch to help meet security and compliance needs, and recommends storing them in S3 for analysis with Athena, encrypting them, and moving to Glacier for cost savings.
-  Amazon GuardDuty is an intelligent threat detection service that uses machine learning to analyze CloudTrail, VPC, and DNS logs for unusual behavior and alerts you via EventBridge without needing any software installation.

### 328. Trusted Advisor
- AWS Trusted Advisor is a service that helps you review your AWS account and gives recommendations in six key areas—cost, performance, security, fault tolerance, service limits, and operational excellence—without needing to install anything.

### 329. Encryption in flight (TLS/SSL)
- **Encryption in flight (TLS/SSL)** protects data by encrypting it before transmission and decrypting it upon arrival, ensuring secure communication over HTTPS and preventing man-in-the-middle (MITM) attacks.
- **Server-side encryption at rest** means data is encrypted by the server after it receives it and stored securely using a data key, which is also used to decrypt the data before sending it out again.

### 330. Client-side encryption
- **Client-side encryption** means the data is encrypted by the client before being sent to the server and only decrypted by the receiving client, so the server can never access or read the data.

### 331. AWS Key Management Service
- AWS Key Management Service (KMS) helps you securely create, store, and manage encryption keys, is integrated with IAM and CloudTrail, and is used by many AWS services to control and audit access to encrypted data.
- KMS keys are of two types: **symmetric keys** use one hidden key for both locking and unlocking data through AWS only, while **asymmetric keys** use a public key to lock data and a private key to unlock it, useful even outside AWS.

### 332. KMS Key Policies 
- KMS Key Policies control who can use or manage KMS keys, with default policies giving full access to the root user and custom policies allowing specific users, roles, and cross-account access.
- AWS KMS offers free AWS-owned and AWS-managed keys, while customer-managed and imported keys cost $1/month plus API call fees, with key rotation automatic for AWS-managed keys, optional for customer-managed, and manual for imported keys.

### 333. Copying encrypted snapshots across AWS accounts
- To copy kms encrypted ebs snapshots across AWS accounts, you must attach a cross-account KMS key policy, share the snapshot, then re-encrypt it in the target account before creating a volume.
- You need to re-encrypt the EBS snapshot in the target account not because you're decrypting the data, but because AWS does not allow the target account to use your original KMS key. KMS-encrypted snapshots are tightly bound to the KMS key used to encrypt them, which lives in the source account.
- So instead of decrypting, AWS internally does a re-encryption: it takes the encrypted snapshot and makes a new encrypted copy using a KMS key from the target account. This allows the target account to use the snapshot securely with its own permissions.

### 334. Understanding KMS Key Rotation Options
- AWS-managed keys rotate automatically every year. For your own keys (customer-managed), you can also turn on automatic rotation, which changes the internal part of the key every 365 days but keeps the same key ID so apps still work.
You can also manually rotate your key any time with On-Demand Rotation, but only a few times per year.
- If you want to fully replace a key (like changing the key ID), you can do manual rotation by creating a new key and updating the alias to point to the new one. This way, your app keeps using the alias and doesn’t need any changes.